{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93068dce9f22b25b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Project Introduction:\n",
    "Primer laboratorio de aprendizaje automático, desarrollado por Sergio Barragán Blanco (100472343) y Eduardo Alarcón Navarro (100472175). \n",
    "Grupo 17.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. EDA\n",
    "Existen 22 carácterísticas que definen cada momento, de las cuales ninguna es categórica, todas son numéricas (con la energía suman 23). No existen valores faltantes, pero si los hubiera, los rellenaríamos con la media del valor superior e inferior antes de randomizar el dataset.\n",
    "\n",
    "No existen tampoco columnas constantes, que se eliminarían. \n",
    "\n",
    "Con todo esto, podemos observar que es un problema de regresión.\n",
    "\n",
    "La variable que estamos intentando predecir es la \"energía\" que es el valor de la energía generada 24 horas después. \n",
    "\n",
    "Por lo tanto, vamos a comenzar con todos los imports necesarios para este proyecto"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "933f5cb4b3926278"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, Normalizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold, RandomizedSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor  \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import neighbors\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-13T07:28:58.421310400Z",
     "start_time": "2024-03-13T07:28:51.886989600Z"
    }
   },
   "id": "e738704b6e206784"
  },
  {
   "cell_type": "markdown",
   "source": [
    "También, vamos a importar el documento con los datos de entrenamiento, y le vamos a eliminar las columnas innecesarias"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a8686906c824078d"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "718a0bf13cf1f98b",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-13T07:29:28.980845100Z",
     "start_time": "2024-03-13T07:29:26.853176300Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('wind_ava.csv.gz', compression='gzip')\n",
    "\n",
    "# FIlter the data to only include the columns that end in 13\n",
    "data = data.filter(regex='13$|energy')\n",
    "#print(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Y separar los datos de entrenamiento de los datos de test, con train_test_split"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7c9742cebf2d115e"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97f5d899342eb290",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-13T07:29:32.192588100Z",
     "start_time": "2024-03-13T07:29:32.155921700Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = data['energy']\n",
    "x = data.drop(columns=['energy'])\n",
    "\n",
    "# test_size vamos a escoger 0.2 ya que tenemos datos en un rango de 5 años en orden (2015-2019) por lo que (asumiendo que el numero de datos para todos los años es similar) usaremos 2019 como test final.\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, shuffle=False)\n",
    "#print(f\"Fechas de tain {train.iloc[0].datetime}-{train-iloc[-1].datetime}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "4d0ecc92bc45e922"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fc2e646359c0bd8",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-13T07:29:40.307322800Z",
     "start_time": "2024-03-13T07:29:40.275863700Z"
    }
   },
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import KFold, RandomizedSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier  \n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "scalers = [StandardScaler(), MinMaxScaler(), RobustScaler(), Normalizer(), PowerTransformer]\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), x.columns)\n",
    "    ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Decidir usando KNN\n",
    "A continuación, usaremos KNN para determinar el método de escalado más apropiado para el problema. \n",
    "Dado que no queremos entrenar con datos en el futuro, no podremos usar una validacin cruzada normal (usando Kfold o Shuffle), por lo que tendremos que usar predefined split o Time Series split (el cual es igual a una validación cruzada, pero ignora los posteriores a test), en nuestro caso, probaremos con.......................................................................................................................................................................................................................\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "54e98abb555e4f64"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot clone object. You should provide an instance of scikit-learn estimator instead of a class.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 23\u001B[0m\n\u001B[0;32m     15\u001B[0m model \u001B[38;5;241m=\u001B[39m GridSearchCV(KNeighborsRegressor,  \u001B[38;5;66;03m# Use DecisionTreeRegressor\u001B[39;00m\n\u001B[0;32m     16\u001B[0m                      param_grid, cv\u001B[38;5;241m=\u001B[39mcv, n_jobs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, scoring\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mneg_mean_absolute_percentage_error\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     18\u001B[0m pipe \u001B[38;5;241m=\u001B[39m Pipeline([\n\u001B[0;32m     19\u001B[0m     (\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpreprocessor\u001B[39m\u001B[38;5;124m'\u001B[39m, preprocessor),\n\u001B[0;32m     20\u001B[0m     (\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m'\u001B[39m, model)\n\u001B[0;32m     21\u001B[0m ])\n\u001B[1;32m---> 23\u001B[0m \u001B[43mpipe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     25\u001B[0m best_model \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mbest_estimator_\n\u001B[0;32m     26\u001B[0m best_params \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mbest_params_\n",
      "File \u001B[1;32m~\\OneDrive\\Escritorio\\Uni\\Tercero Uni\\AA-LAB1\\venv\\lib\\site-packages\\sklearn\\base.py:1474\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[1;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1467\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[0;32m   1469\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m   1470\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m   1471\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m   1472\u001B[0m     )\n\u001B[0;32m   1473\u001B[0m ):\n\u001B[1;32m-> 1474\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fit_method(estimator, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\OneDrive\\Escritorio\\Uni\\Tercero Uni\\AA-LAB1\\venv\\lib\\site-packages\\sklearn\\pipeline.py:475\u001B[0m, in \u001B[0;36mPipeline.fit\u001B[1;34m(self, X, y, **params)\u001B[0m\n\u001B[0;32m    473\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_final_estimator \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpassthrough\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    474\u001B[0m         last_step_params \u001B[38;5;241m=\u001B[39m routed_params[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msteps[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m][\u001B[38;5;241m0\u001B[39m]]\n\u001B[1;32m--> 475\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_final_estimator\u001B[38;5;241m.\u001B[39mfit(Xt, y, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mlast_step_params[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfit\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[0;32m    477\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "File \u001B[1;32m~\\OneDrive\\Escritorio\\Uni\\Tercero Uni\\AA-LAB1\\venv\\lib\\site-packages\\sklearn\\base.py:1474\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[1;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1467\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[0;32m   1469\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m   1470\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m   1471\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m   1472\u001B[0m     )\n\u001B[0;32m   1473\u001B[0m ):\n\u001B[1;32m-> 1474\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fit_method(estimator, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\OneDrive\\Escritorio\\Uni\\Tercero Uni\\AA-LAB1\\venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:882\u001B[0m, in \u001B[0;36mBaseSearchCV.fit\u001B[1;34m(self, X, y, **params)\u001B[0m\n\u001B[0;32m    879\u001B[0m cv_orig \u001B[38;5;241m=\u001B[39m check_cv(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcv, y, classifier\u001B[38;5;241m=\u001B[39mis_classifier(estimator))\n\u001B[0;32m    880\u001B[0m n_splits \u001B[38;5;241m=\u001B[39m cv_orig\u001B[38;5;241m.\u001B[39mget_n_splits(X, y, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mrouted_params\u001B[38;5;241m.\u001B[39msplitter\u001B[38;5;241m.\u001B[39msplit)\n\u001B[1;32m--> 882\u001B[0m base_estimator \u001B[38;5;241m=\u001B[39m \u001B[43mclone\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mestimator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    884\u001B[0m parallel \u001B[38;5;241m=\u001B[39m Parallel(n_jobs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_jobs, pre_dispatch\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpre_dispatch)\n\u001B[0;32m    886\u001B[0m fit_and_score_kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m(\n\u001B[0;32m    887\u001B[0m     scorer\u001B[38;5;241m=\u001B[39mscorers,\n\u001B[0;32m    888\u001B[0m     fit_params\u001B[38;5;241m=\u001B[39mrouted_params\u001B[38;5;241m.\u001B[39mestimator\u001B[38;5;241m.\u001B[39mfit,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    895\u001B[0m     verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose,\n\u001B[0;32m    896\u001B[0m )\n",
      "File \u001B[1;32m~\\OneDrive\\Escritorio\\Uni\\Tercero Uni\\AA-LAB1\\venv\\lib\\site-packages\\sklearn\\base.py:91\u001B[0m, in \u001B[0;36mclone\u001B[1;34m(estimator, safe)\u001B[0m\n\u001B[0;32m     89\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(estimator, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__sklearn_clone__\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m inspect\u001B[38;5;241m.\u001B[39misclass(estimator):\n\u001B[0;32m     90\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m estimator\u001B[38;5;241m.\u001B[39m__sklearn_clone__()\n\u001B[1;32m---> 91\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_clone_parametrized\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msafe\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msafe\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\OneDrive\\Escritorio\\Uni\\Tercero Uni\\AA-LAB1\\venv\\lib\\site-packages\\sklearn\\base.py:107\u001B[0m, in \u001B[0;36m_clone_parametrized\u001B[1;34m(estimator, safe)\u001B[0m\n\u001B[0;32m    105\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    106\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(estimator, \u001B[38;5;28mtype\u001B[39m):\n\u001B[1;32m--> 107\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[0;32m    108\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot clone object. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    109\u001B[0m             \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou should provide an instance of \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    110\u001B[0m             \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mscikit-learn estimator instead of a class.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    111\u001B[0m         )\n\u001B[0;32m    112\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    113\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[0;32m    114\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot clone object \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m (type \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m): \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    115\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mit does not seem to be a scikit-learn \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    116\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mestimator as it does not implement a \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    117\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mget_params\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m method.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m (\u001B[38;5;28mrepr\u001B[39m(estimator), \u001B[38;5;28mtype\u001B[39m(estimator))\n\u001B[0;32m    118\u001B[0m         )\n",
      "\u001B[1;31mTypeError\u001B[0m: Cannot clone object. You should provide an instance of scikit-learn estimator instead of a class."
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# Usaremos los parametros por default n_slits = 5, max_train_dize = None, test_size = None, gap = 0 \n",
    "cv = TimeSeriesSplit()\n",
    "\n",
    "param_grid = {\n",
    "    'n_neighbors': [5, 10, 15, 20], \n",
    "    'weights' : ['uniform', 'distance'],\n",
    "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "    'leaf_size': [30, 40, 50],\n",
    "    'p' : [2], # distance `= 1 manhatan, p=2 euclidean\n",
    "}\n",
    "\n",
    "model = GridSearchCV(KNeighborsRegressor,\n",
    "                     param_grid, cv=cv, n_jobs=-1, scoring='neg_mean_absolute_percentage_error')\n",
    "print(model)\n",
    "pipe = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', model)\n",
    "])\n",
    "print(pipe)\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "best_model = model.best_estimator_\n",
    "best_params = model.best_params_\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"Mejor modelo:\",best_model)\n",
    "print(\"Mejorres parámetros:\",best_params)\n",
    "print(\"RMSE:\", rmse)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-13T07:40:38.908195600Z",
     "start_time": "2024-03-13T07:40:38.790261400Z"
    }
   },
   "id": "d318af744d9aa25a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Entrenamiento de diferentes modelos\n",
    "Lo próximo que haremos, será probar con diferentes métodos de entrenamiento (KNN, árboles de regresión, regresión lineal normal y la variante de Lasso, y SVM. Para elegir el mejor, usaremos la librería time para saber cuánto se tarda en entrenar cada modelo, un dummy para comparar resultados, y el mejor de entre ellos, usando una política de importancia de 20% a tiempo y 80% precisión.\n",
    "\n",
    "Lo primero que vamos a usar es el dummyRegressor, para tener una representación visual del peor escenario posible y que tanto mejoran nuestros diseños."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9dcecc08219a0344"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f115f6a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T08:01:36.239271400Z",
     "start_time": "2024-03-13T08:01:36.207995300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 451489.08466268575\n",
      "SQRT RMSE: 671.9293747580067\n"
     ]
    }
   ],
   "source": [
    "# Create a Dunny regressor to obtain a baseline\n",
    "\n",
    "from sklearn.dummy import DummyRegressor\n",
    "dummy = DummyRegressor(strategy='mean')\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', dummy)\n",
    "])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "y_pred = pipe.predict(X_test)\n",
    "\n",
    "rmse_dummy = metrics.mean_squared_error(y_test, y_pred)\n",
    "sqrt_rmse_dummy = np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
    "print(f\"RMSE: {rmse_dummy}\")\n",
    "print(f\"SQRT RMSE: {sqrt_rmse_dummy}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Para empezar, vamos a comparar con un Decision Tree Regresor."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "595af26d8e4e2799"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Optimizing hyperparameters\n",
    "from sklearn.model_selection import GridSearchCV, KFold, RandomizedSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor  # Import DecisionTreeRegressor instead of DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'criterion': ['friedman_mse', 'absolute_error', 'poisson', 'squared_error'],  # Adjust criterion for regression\n",
    "    'max_depth': [None, 12, 13, 14, 15, 16, 17],\n",
    "    'min_samples_split': range(2, 15, 2),\n",
    "    'min_samples_leaf': range(5, 15, 2),\n",
    "}\n",
    "\n",
    "cv = TimeSeriesSplit()\n",
    "model = RandomizedSearchCV(DecisionTreeRegressor(random_state=7543),  # Use DecisionTreeRegressor\n",
    "                     param_grid, cv=cv, n_jobs=-1, verbose=1, scoring='neg_mean_absolute_percentage_error', n_iter=50)  # Adjust scoring metric\n",
    "\n",
    "model2 = GridSearchCV(DecisionTreeRegressor(random_state=7543),  # Use DecisionTreeRegressor\n",
    "                     param_grid, cv=cv, n_jobs=-1, verbose=1, scoring='neg_mean_absolute_percentage_error')\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', model)\n",
    "])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "y_pred = pipe.predict(X_test)\n",
    "\n",
    "\n",
    "rmse_tree = metrics.mean_squared_error(y_test, y_pred)\n",
    "sqrt_rmse_tree = np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
    "print(f\"RMSE: {rmse_tree}\")\n",
    "print(f\"SQRT RMSE: {sqrt_rmse_tree}\")\n",
    "print(f\"Best params: {model.best_params_}\")\n",
    "# Best params: {'criterion': 'absolute_error', 'max_depth': 15, 'min_samples_leaf': 11, 'min_samples_split': 3}\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "48f24c60f565a7dc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Como hemos podido observar, el modelo es bastante mejor que el dummy, aproximadamente, un tercio mejor. No obstante probaremos con más modelos para contrastar.\n",
    "\n",
    "El próximo modelo es la regresión lineal normal"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aba3270d69a5b0f4"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f275a987e9421aae",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-13T07:48:21.264541100Z",
     "start_time": "2024-03-13T07:48:20.916441600Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrador\\OneDrive\\Escritorio\\Uni\\Tercero Uni\\AA-LAB1\\venv\\lib\\site-packages\\scipy\\optimize\\_nnls.py:135: LinAlgWarning: Ill-conditioned matrix (rcond=1.82501e-17): result may not be accurate.\n",
      "  s[P] = solve(AtA[P_ind[:, None], P_ind[None, :]], Atb[P],\n",
      "C:\\Users\\Administrador\\OneDrive\\Escritorio\\Uni\\Tercero Uni\\AA-LAB1\\venv\\lib\\site-packages\\scipy\\optimize\\_nnls.py:135: LinAlgWarning: Ill-conditioned matrix (rcond=2.06611e-17): result may not be accurate.\n",
      "  s[P] = solve(AtA[P_ind[:, None], P_ind[None, :]], Atb[P],\n",
      "C:\\Users\\Administrador\\OneDrive\\Escritorio\\Uni\\Tercero Uni\\AA-LAB1\\venv\\lib\\site-packages\\scipy\\optimize\\_nnls.py:135: LinAlgWarning: Ill-conditioned matrix (rcond=2.24549e-17): result may not be accurate.\n",
      "  s[P] = solve(AtA[P_ind[:, None], P_ind[None, :]], Atb[P],\n",
      "C:\\Users\\Administrador\\OneDrive\\Escritorio\\Uni\\Tercero Uni\\AA-LAB1\\venv\\lib\\site-packages\\scipy\\optimize\\_nnls.py:135: LinAlgWarning: Ill-conditioned matrix (rcond=2.33915e-17): result may not be accurate.\n",
      "  s[P] = solve(AtA[P_ind[:, None], P_ind[None, :]], Atb[P],\n",
      "C:\\Users\\Administrador\\OneDrive\\Escritorio\\Uni\\Tercero Uni\\AA-LAB1\\venv\\lib\\site-packages\\scipy\\optimize\\_nnls.py:135: LinAlgWarning: Ill-conditioned matrix (rcond=2.26895e-17): result may not be accurate.\n",
      "  s[P] = solve(AtA[P_ind[:, None], P_ind[None, :]], Atb[P],\n",
      "C:\\Users\\Administrador\\OneDrive\\Escritorio\\Uni\\Tercero Uni\\AA-LAB1\\venv\\lib\\site-packages\\scipy\\optimize\\_nnls.py:135: LinAlgWarning: Ill-conditioned matrix (rcond=1.2106e-17): result may not be accurate.\n",
      "  s[P] = solve(AtA[P_ind[:, None], P_ind[None, :]], Atb[P],\n",
      "C:\\Users\\Administrador\\OneDrive\\Escritorio\\Uni\\Tercero Uni\\AA-LAB1\\venv\\lib\\site-packages\\scipy\\optimize\\_nnls.py:135: LinAlgWarning: Ill-conditioned matrix (rcond=2.47776e-17): result may not be accurate.\n",
      "  s[P] = solve(AtA[P_ind[:, None], P_ind[None, :]], Atb[P],\n",
      "C:\\Users\\Administrador\\OneDrive\\Escritorio\\Uni\\Tercero Uni\\AA-LAB1\\venv\\lib\\site-packages\\scipy\\optimize\\_nnls.py:135: LinAlgWarning: Ill-conditioned matrix (rcond=1.09778e-17): result may not be accurate.\n",
      "  s[P] = solve(AtA[P_ind[:, None], P_ind[None, :]], Atb[P],\n",
      "C:\\Users\\Administrador\\OneDrive\\Escritorio\\Uni\\Tercero Uni\\AA-LAB1\\venv\\lib\\site-packages\\scipy\\optimize\\_nnls.py:135: LinAlgWarning: Ill-conditioned matrix (rcond=1.14504e-17): result may not be accurate.\n",
      "  s[P] = solve(AtA[P_ind[:, None], P_ind[None, :]], Atb[P],\n",
      "C:\\Users\\Administrador\\OneDrive\\Escritorio\\Uni\\Tercero Uni\\AA-LAB1\\venv\\lib\\site-packages\\scipy\\optimize\\_nnls.py:135: LinAlgWarning: Ill-conditioned matrix (rcond=1.15046e-17): result may not be accurate.\n",
      "  s[P] = solve(AtA[P_ind[:, None], P_ind[None, :]], Atb[P],\n",
      "C:\\Users\\Administrador\\OneDrive\\Escritorio\\Uni\\Tercero Uni\\AA-LAB1\\venv\\lib\\site-packages\\scipy\\optimize\\_nnls.py:135: LinAlgWarning: Ill-conditioned matrix (rcond=1.1129e-17): result may not be accurate.\n",
      "  s[P] = solve(AtA[P_ind[:, None], P_ind[None, :]], Atb[P],\n",
      "C:\\Users\\Administrador\\OneDrive\\Escritorio\\Uni\\Tercero Uni\\AA-LAB1\\venv\\lib\\site-packages\\scipy\\optimize\\_nnls.py:135: LinAlgWarning: Ill-conditioned matrix (rcond=1.82501e-17): result may not be accurate.\n",
      "  s[P] = solve(AtA[P_ind[:, None], P_ind[None, :]], Atb[P],\n",
      "C:\\Users\\Administrador\\OneDrive\\Escritorio\\Uni\\Tercero Uni\\AA-LAB1\\venv\\lib\\site-packages\\scipy\\optimize\\_nnls.py:135: LinAlgWarning: Ill-conditioned matrix (rcond=2.06611e-17): result may not be accurate.\n",
      "  s[P] = solve(AtA[P_ind[:, None], P_ind[None, :]], Atb[P],\n",
      "C:\\Users\\Administrador\\OneDrive\\Escritorio\\Uni\\Tercero Uni\\AA-LAB1\\venv\\lib\\site-packages\\scipy\\optimize\\_nnls.py:135: LinAlgWarning: Ill-conditioned matrix (rcond=2.24549e-17): result may not be accurate.\n",
      "  s[P] = solve(AtA[P_ind[:, None], P_ind[None, :]], Atb[P],\n",
      "C:\\Users\\Administrador\\OneDrive\\Escritorio\\Uni\\Tercero Uni\\AA-LAB1\\venv\\lib\\site-packages\\scipy\\optimize\\_nnls.py:135: LinAlgWarning: Ill-conditioned matrix (rcond=2.33915e-17): result may not be accurate.\n",
      "  s[P] = solve(AtA[P_ind[:, None], P_ind[None, :]], Atb[P],\n",
      "C:\\Users\\Administrador\\OneDrive\\Escritorio\\Uni\\Tercero Uni\\AA-LAB1\\venv\\lib\\site-packages\\scipy\\optimize\\_nnls.py:135: LinAlgWarning: Ill-conditioned matrix (rcond=2.26895e-17): result may not be accurate.\n",
      "  s[P] = solve(AtA[P_ind[:, None], P_ind[None, :]], Atb[P],\n",
      "C:\\Users\\Administrador\\OneDrive\\Escritorio\\Uni\\Tercero Uni\\AA-LAB1\\venv\\lib\\site-packages\\scipy\\optimize\\_nnls.py:135: LinAlgWarning: Ill-conditioned matrix (rcond=1.2106e-17): result may not be accurate.\n",
      "  s[P] = solve(AtA[P_ind[:, None], P_ind[None, :]], Atb[P],\n",
      "C:\\Users\\Administrador\\OneDrive\\Escritorio\\Uni\\Tercero Uni\\AA-LAB1\\venv\\lib\\site-packages\\scipy\\optimize\\_nnls.py:135: LinAlgWarning: Ill-conditioned matrix (rcond=2.47776e-17): result may not be accurate.\n",
      "  s[P] = solve(AtA[P_ind[:, None], P_ind[None, :]], Atb[P],\n",
      "C:\\Users\\Administrador\\OneDrive\\Escritorio\\Uni\\Tercero Uni\\AA-LAB1\\venv\\lib\\site-packages\\scipy\\optimize\\_nnls.py:135: LinAlgWarning: Ill-conditioned matrix (rcond=1.09778e-17): result may not be accurate.\n",
      "  s[P] = solve(AtA[P_ind[:, None], P_ind[None, :]], Atb[P],\n",
      "C:\\Users\\Administrador\\OneDrive\\Escritorio\\Uni\\Tercero Uni\\AA-LAB1\\venv\\lib\\site-packages\\scipy\\optimize\\_nnls.py:135: LinAlgWarning: Ill-conditioned matrix (rcond=1.14504e-17): result may not be accurate.\n",
      "  s[P] = solve(AtA[P_ind[:, None], P_ind[None, :]], Atb[P],\n",
      "C:\\Users\\Administrador\\OneDrive\\Escritorio\\Uni\\Tercero Uni\\AA-LAB1\\venv\\lib\\site-packages\\scipy\\optimize\\_nnls.py:135: LinAlgWarning: Ill-conditioned matrix (rcond=1.15046e-17): result may not be accurate.\n",
      "  s[P] = solve(AtA[P_ind[:, None], P_ind[None, :]], Atb[P],\n",
      "C:\\Users\\Administrador\\OneDrive\\Escritorio\\Uni\\Tercero Uni\\AA-LAB1\\venv\\lib\\site-packages\\scipy\\optimize\\_nnls.py:135: LinAlgWarning: Ill-conditioned matrix (rcond=1.1129e-17): result may not be accurate.\n",
      "  s[P] = solve(AtA[P_ind[:, None], P_ind[None, :]], Atb[P],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor modelo: LinearRegression(n_jobs=-1)\n",
      "Mejorres parámetros: {'copy_X': True, 'fit_intercept': True, 'n_jobs': -1, 'positive': False}\n",
      "RMSE: 559.7095305382695\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'fit_intercept': [True, False],\n",
    "    'copy_X': [True, False],\n",
    "    'n_jobs': [-1],  # -1 indica utilizar todos los núcleos\n",
    "    'positive': [True, False]\n",
    "}\n",
    "\n",
    "\n",
    "# Crear el modelo de regresión lineal\n",
    "model = LinearRegression()\n",
    "cv = TimeSeriesSplit()\n",
    "\n",
    "grid_search = GridSearchCV(model,\n",
    "                           param_grid,\n",
    "                           cv=cv,\n",
    "                           scoring=\"neg_mean_absolute_percentage_error\"\n",
    "                           )\n",
    "\n",
    "# Entrenar el modelo con los datos de entrenamiento\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"Mejor modelo:\",best_model)\n",
    "print(\"Mejorres parámetros:\",best_params)\n",
    "print(\"RMSE:\", rmse)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Como podemos observar, aunque es mejor que el dummy, está bastante peor que el árbol de decisión, no obstante, vamos a probar con  con la variante de Lasso a ver si conseguimos mejorar el entrenamiento:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b3135ff92e842b77"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor modelo: Lasso(alpha=0.1, precompute=True, random_state=7543, selection='random',\n",
      "      tol=1.0, warm_start=True)\n",
      "Mejorres parámetros: {'alpha': 0.1, 'copy_X': True, 'fit_intercept': True, 'positive': False, 'precompute': True, 'random_state': 7543, 'selection': 'random', 'tol': 1.0, 'warm_start': True}\n",
      "RMSE: 567.7651275011757\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Definir los parámetros que deseas probar\n",
    "parameters = {\n",
    "    'alpha': [0.1, 0.5, 1.0, 2.0],  # Valores de regularización\n",
    "    'fit_intercept': [True, False],\n",
    "    'copy_X': [True, False], # pendientes de borrar\n",
    "    'positive': [True, False], \n",
    "    'precompute': [True, False], \n",
    "    'random_state': [7543], \n",
    "    'selection': ['cyclic', 'random'], \n",
    "    'tol': [1**(-10), 1**(-5), 1**(-4), 1**(-3), 1**(-2)], \n",
    "    'warm_start': [True, False]\n",
    "}\n",
    "\n",
    "# Crear el modelo de regresión lineal Lasso\n",
    "model = Lasso()\n",
    "cv = TimeSeriesSplit()\n",
    "grid_search = GridSearchCV(model, \n",
    "                           parameters, \n",
    "                           scoring='neg_mean_squared_error', \n",
    "                           cv=cv)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Obtener el mejor modelo y sus hiperparámetros\n",
    "best_model = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"Mejor modelo:\",best_model)\n",
    "print(\"Mejorres parámetros:\",best_params)\n",
    "print(\"RMSE:\", rmse)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-13T07:49:07.993811Z",
     "start_time": "2024-03-13T07:48:40.299035100Z"
    }
   },
   "id": "2bc265db12c228b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Afortunadamente, el modelo es mejor que la regresión lineal normal, pero los árboles le sacan bastante más precisión (aunque tardan más en entrenar, el beneficio sobrepasa los costes.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "609f3ff6c09b9bfc"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3823beec798adab4",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-13T09:33:32.875592500Z",
     "start_time": "2024-03-13T09:33:26.262228300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor modelo: KNeighborsRegressor(leaf_size=20, n_neighbors=12)\n",
      "Mejorres parámetros: {'algorithm': 'auto', 'leaf_size': 20, 'n_neighbors': 12, 'p': 2, 'weights': 'uniform'}\n",
      "RMSE: 601.0721146249629\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# Usaremos los parametros por default n_slits = 5, max_train_dize = None, test_size = None, gap = 0 \n",
    "cv = TimeSeriesSplit()\n",
    "\n",
    "param_grid = {\n",
    "    'n_neighbors': range(7, 15, 1), \n",
    "    'weights' : ['uniform', 'distance'],\n",
    "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "    'leaf_size': range(20, 40, 5),\n",
    "    'p' : [2], # distance `= 1 manhatan, p=2 euclidean\n",
    "}\n",
    "\n",
    "model = KNeighborsRegressor()\n",
    "grid_search = GridSearchCV(model,\n",
    "                     param_grid, cv=cv, n_jobs=-1, scoring='neg_mean_absolute_percentage_error')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "y_pred = grid_search.predict(X_test)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"Mejor modelo:\",best_model)\n",
    "print(\"Mejorres parámetros:\",best_params)\n",
    "print(\"RMSE:\", rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Los últimos modelos que vamos a entrenar van a ser KNN, SVM y redes de neuronas"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c0b2fb7cbea45829"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Correlaciones entre parámetros:\n",
    "Al principio, parece que las columnas lai_lv.13 y lai_hv.13 tienen una correlación, pero según avanza el tiempo, desaparece.\n",
    "\n",
    "## Escala de los datos\n",
    "Entre las diferentes columnas de datos, tenemos valores y magnitudes muy dispares."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fe1707edb484fdf4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
