{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93068dce9f22b25b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Project Introduction:\n",
    "Primer laboratorio de aprendizaje automático, desarrollado por Sergio Barragán Blanco (100472343) y Eduardo Alarcón Navarro (100472175). \n",
    "Grupo 17.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933f5cb4b3926278",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 1. Problema de Clasificación\n",
    "A continuación, dividiremos la energía entre alta y baja siguiendo las instrucciones (alta si es mayor que el tercer quartil). Y entrenaremos el mejor modelo anterior para ver los resultados, posteriormente elegiremos algunos modelos de Clasificación para entrenar este nuevo problema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e738704b6e206784",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T16:15:28.883011600Z",
     "start_time": "2024-04-11T16:15:22.151084900Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.preprocessing import PowerTransformer, RobustScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, TimeSeriesSplit, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVR, SVC\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, precision_score, recall_score, f1_score, balanced_accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8686906c824078d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "También, vamos a importar el documento con los datos de entrenamiento, y le vamos a eliminar las columnas innecesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "718a0bf13cf1f98b",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T16:15:33.341759600Z",
     "start_time": "2024-04-11T16:15:31.620933500Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('wind_ava.csv.gz', compression='gzip')\n",
    "\n",
    "# FIlter the data to only include the columns that end in 13\n",
    "data = data.filter(regex='13$|energy')\n",
    "#print(data)\n",
    "\n",
    "y = data['energy']\n",
    "x = data.drop(columns=['energy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e98abb555e4f64",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d318af744d9aa25a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test_size vamos a escoger 0.2 ya que tenemos datos en un rango de 5 años en orden (2015-2019) por lo que (asumiendo que el numero de datos para todos los años es similar) usaremos 2019 como test final.\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, shuffle=False)\n",
    "#print(f\"Fechas de tain {train.iloc[0].datetime}-{train-iloc[-1].datetime}\")\n",
    "\n",
    "model = SVR(C=701, degree=2, gamma='auto', kernel='rbf', epsilon=0.1)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('scaler', PowerTransformer()),\n",
    "    ('model', model)\n",
    "])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipe.predict(X_test)\n",
    "# Comprobamos que la RMSE sea igual que en el ejemplo anterior\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"RMSE:\", rmse)\n",
    "\n",
    "# Ahora vamos a clasificar entre alta y baja, a ver que tan preciso es\n",
    "tercer_cuartil = np.quantile(y_train, 0.75)\n",
    "\n",
    "# Asignar etiquetas\n",
    "y_test_class = np.where(y_test > tercer_cuartil, 1, 0)\n",
    "\n",
    "# Predecir valores\n",
    "y_pred_class = np.where(y_pred > tercer_cuartil, 1, 0)\n",
    "\n",
    "# Evaluar la clasificación\n",
    "accuracy = accuracy_score(y_test_class, y_pred_class)\n",
    "precision = precision_score(y_test_class, y_pred_class, pos_label=1)\n",
    "recall = recall_score(y_test_class, y_pred_class, pos_label=1)\n",
    "f1_alta = f1_score(y_test_class, y_pred_class, pos_label=1)\n",
    "f1_baja = f1_score(y_test_class, y_pred_class, pos_label=0)\n",
    "balanced_accuracy = balanced_accuracy_score(y_test_class, y_pred_class)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", \"ALTA = \", f1_alta, \"BAJA = \", f1_baja)\n",
    "print(\"Balanced Accuracy:\", balanced_accuracy)\n",
    "\n",
    "# Matriz de confusión\n",
    "confusion_matrix = pd.crosstab(y_test_class, y_pred_class, rownames=['Real'], colnames=['Predicho'])\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616304f5b8e8f474",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Dado que el resultado más preciso para nuestro modelo ha sido la accuracy, la mantendremos como principal estimación para futuros modelos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcecc08219a0344",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Modelos que usan clasificación\n",
    "A continuación transformaremos los datos para que los valores a predecir sean de clasificación. Debido a que el PowerTransformer() no funciona bien para clasificación, en su defecto, usaremos el segundo mejor scaler, el RobustScaler().\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f115f6a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T16:23:22.184276200Z",
     "start_time": "2024-04-11T16:23:22.135256300Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Paso 3: Entrenamiento del modelo\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "tercer_cuantil = np.quantile(y_train, 0.75)\n",
    "y_train = np.where(y_train < tercer_cuantil, 0, 1)\n",
    "y_test = np.where(y_test < tercer_cuantil, 0, 1)\n",
    "\n",
    "# 1 Alta\n",
    "# 0 Baja\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595af26d8e4e2799",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Primero probaremos con el KNN Clasifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc97d045b7d6ce8b",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T16:25:37.655813100Z",
     "start_time": "2024-04-11T16:24:44.184993200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor modelo: KNeighborsClassifier(leaf_size=1, n_neighbors=15, p=1, weights='distance') <---------> KNeighborsClassifier()\n",
      "Mejores parámetros: {'algorithm': 'auto', 'leaf_size': 1, 'n_neighbors': 15, 'p': 1, 'weights': 'distance'} <---------> {}\n",
      "Score: 0.784670463832614 <---------> 0.7742702702415876\n"
     ]
    },
    {
     "data": {
      "text/plain": "\"\\nMejor modelo: KNeighborsClassifier(leaf_size=1, n_neighbors=15, p=1, weights='distance')\\nMejores parámetros: {'algorithm': 'auto', 'leaf_size': 1, 'n_neighbors': 15, 'p': 1, 'weights': 'distance'}\\nScore: 0.784670463832614\\n\""
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_neighbors': range(5, 40, 5), \n",
    "    'weights' : ['uniform', 'distance'],\n",
    "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "    'leaf_size': range(1, 25, 5),\n",
    "    'p' : [1, 2]  # distance `= 1 manhattan, p=2 euclidean\n",
    "}\n",
    "\n",
    "cv = TimeSeriesSplit()\n",
    "\n",
    "knn_model = KNeighborsClassifier()\n",
    "\n",
    "# Búsqueda de hiperparámetros\n",
    "grid_search = GridSearchCV(knn_model, param_grid, cv=cv, n_jobs=-1, scoring='f1_macro')\n",
    "without_params = GridSearchCV(knn_model,{}, cv=cv, n_jobs=-1, scoring='f1_macro')\n",
    "\n",
    "# Construcción del pipeline\n",
    "pipe = Pipeline([\n",
    "    ('scaler', RobustScaler()),\n",
    "    ('model', grid_search)\n",
    "])\n",
    "pipe_without_params = Pipeline([\n",
    "    ('scaler', RobustScaler()),\n",
    "    ('model', without_params)\n",
    "])\n",
    "\n",
    "# Entrenamiento del pipeline\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe_without_params.fit(X_train, y_train)\n",
    "\n",
    "# Mejor modelo y parámetros\n",
    "best_model = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "best_model_w = without_params.best_estimator_\n",
    "best_params_w = without_params.best_params_\n",
    "\n",
    "print(\"Mejor modelo:\", best_model, \"<--------->\", best_model_w)\n",
    "print(\"Mejores parámetros:\", best_params, \"<--------->\", best_params_w)\n",
    "print(\"Score:\", grid_search.best_score_, \"<--------->\", without_params.best_score_)\n",
    "\n",
    "\"\"\"\n",
    "Mejor modelo: KNeighborsClassifier(leaf_size=1, n_neighbors=15, p=1, weights='distance') <---------> KNeighborsClassifier()\n",
    "Mejores parámetros: {'algorithm': 'auto', 'leaf_size': 1, 'n_neighbors': 15, 'p': 1, 'weights': 'distance'} <---------> {}\n",
    "Score: 0.784670463832614 <---------> 0.7742702702415876\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9eae371d5a49e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "A continuación probaremos con un Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "243b4251e300523",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T16:32:23.098383200Z",
     "start_time": "2024-04-11T16:30:08.866290800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 560 candidates, totalling 2800 fits\n",
      "Fitting 5 folds for each of 560 candidates, totalling 2800 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Mejores parámetros: Sin scaler: {'class_weight': 'balanced', 'criterion': 'gini', 'max_depth': 12, 'min_samples_leaf': 9, 'min_samples_split': 14} <-----> Con scaler: {'class_weight': 'balanced', 'criterion': 'gini', 'max_depth': 12, 'min_samples_leaf': 9, 'min_samples_split': 6} <-----------> Sin HPO{}\n",
      "Mejor puntuación: Sin scaler 0.7887936967961423 <-----> Con scaler: 0.7891987832808501 <-----------> Sin HPO0.7367893170622747\n"
     ]
    },
    {
     "data": {
      "text/plain": "\"\\nMejores parámetros: {'class_weight': 'balanced', 'criterion': 'gini', 'max_depth': 10, 'min_samples_leaf': 9, 'min_samples_split': 8}-----{'class_weight': 'balanced', 'criterion': 'gini', 'max_depth': 10, 'min_samples_leaf': 9, 'min_samples_split': 12}\\nMejor puntuación: 0.7882982356200342-----0.7883933860404928\\n\""
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Definir el espacio de búsqueda de hiperparámetros\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],  # Criterios de desbalanceo para clasificación\n",
    "    'max_depth': [None, 10, 12, 13, 14, 15, 16, 17],\n",
    "    'min_samples_split': range(2, 15, 2),\n",
    "    'min_samples_leaf': range(1, 10, 2),\n",
    "    'class_weight': ['balanced']  \n",
    "}\n",
    "\n",
    "cv = TimeSeriesSplit()\n",
    "\n",
    "model = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=cv, n_jobs=-1, verbose=1,\n",
    "                           scoring='f1_macro')  \n",
    "\n",
    "model2 = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=cv, n_jobs=-1, verbose=1,\n",
    "                      scoring='f1_macro')\n",
    "without_params = GridSearchCV(DecisionTreeClassifier(), {}, cv=cv, n_jobs=-1, verbose=1,\n",
    "                      scoring='f1_macro')\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('scaler', RobustScaler()),\n",
    "    ('model', model),\n",
    "    \n",
    "])\n",
    "pipe_without_params = Pipeline([\n",
    "    ('scaler', RobustScaler()),\n",
    "    ('model', without_params),\n",
    "    \n",
    "])\n",
    "\n",
    "# Entrenamiento del pipeline\n",
    "pipe.fit(X_train, y_train)\n",
    "model2.fit(X_train, y_train)\n",
    "pipe_without_params.fit(X_train, y_train)\n",
    "\n",
    "pipe_best_params = pipe.named_steps['model'].best_params_\n",
    "pipe_score = pipe.named_steps['model'].best_score_\n",
    "\n",
    "pipe_wp_best_params = pipe_without_params.named_steps['model'].best_params_\n",
    "pipe_wp_score = pipe_without_params.named_steps['model'].best_score_\n",
    "\n",
    "best_params = model2.best_params_\n",
    "score = model2.best_score_\n",
    "\n",
    "print(f\"Mejores parámetros: Sin scaler: {best_params} <-----> Con scaler: {pipe_best_params} <-----------> Sin HPO{pipe_wp_best_params}\")\n",
    "print(f\"Mejor puntuación: Sin scaler {score} <-----> Con scaler: {pipe_score} <-----------> Sin HPO: {pipe_wp_score}\")\n",
    "\n",
    "\"\"\"\n",
    "Mejores parámetros: Sin scaler: {'class_weight': 'balanced', 'criterion': 'gini', 'max_depth': 12, 'min_samples_leaf': 9, 'min_samples_split': 14} <-----> Con scaler: {'class_weight': 'balanced', 'criterion': 'gini', 'max_depth': 12, 'min_samples_leaf': 9, 'min_samples_split': 6} <-----------> Sin HPO{}\n",
    "Mejor puntuación: Sin scaler 0.7887936967961423 <-----> Con scaler: 0.7891987832808501 <-----------> Sin HPO: 0.7367893170622747\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a7ee1fb2af7a6b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Y por último, probaremos con un SVM, ya que el mejor modelo para regresión fué su contraparte, esperamos que dé resultados similares o mejores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efec1109a5bb240",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv = TimeSeriesSplit()\n",
    "\n",
    "# Instancia del modelo clasificador SVM\n",
    "model = SVC()\n",
    "\n",
    "# Definir el espacio de búsqueda de hiperparámetros\n",
    "param_grid = {\n",
    "    'C': range(1, 50),\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "    'degree': [2, 3, 4, 5],\n",
    "    'gamma': ['scale', 'auto'],\n",
    "    'class_weight': ['balanced'] \n",
    "}\n",
    "\n",
    "# Búsqueda de hiperparámetros\n",
    "grid_search = GridSearchCV(model,\n",
    "                                 param_grid,\n",
    "                                 cv=cv, \n",
    "                                 n_jobs=-1, \n",
    "                                 scoring='f1_macro',  # Métrica de clasificación\n",
    "                                 )\n",
    "without_params = GridSearchCV(model, {}, cv=cv, n_jobs=-1, scoring='f1_macro')\n",
    "\n",
    "# Construcción del pipeline\n",
    "pipe = Pipeline([\n",
    "    ('scaler', RobustScaler()),\n",
    "    ('model', grid_search)\n",
    "])\n",
    "pipe_without_params = Pipeline([\n",
    "    ('scaler', RobustScaler()),\n",
    "    ('model', without_params)\n",
    "])\n",
    "\n",
    "# Entrenamiento del pipeline\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe_without_params.fit(X_train, y_train)\n",
    "# Obtener el mejor modelo y sus parámetros\n",
    "best_model = pipe.named_steps[\"model\"].best_estimator_\n",
    "best_params = pipe.named_steps[\"model\"].best_params_\n",
    "best_score = pipe.named_steps[\"model\"].best_score_\n",
    "\n",
    "best_model_w = pipe_without_params.named_steps[\"model\"].best_estimator_\n",
    "best_params_w = pipe_without_params.named_steps[\"model\"].best_params_\n",
    "best_score_w = pipe_without_params.named_steps[\"model\"].best_score_\n",
    "\n",
    "print(f\"Mejor modelo: Con HPO: {best_model} <-----> Sin HPO: {best_model_w}\")\n",
    "print(f\"Mejores parámetros: {best_params} <-----> Sin HPO: {best_params_w}\")\n",
    "print(f\"Score: {best_score} <-----> Sin HPO: {best_score_w}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Uso de ChatGPT en esta parte:\n",
    "Para esta parte, como ya poseíamos un amplio conocimento y experiencia de la parte de regresión, no hubo uso de la herramienta. Con la única excepción de intentar resolver el error que daba nan con cualquier scoring debido a que las clases estaban divididas en altas y bajas. No obstante, ChatGPT no solucionó este problema, por lo que se acabó usando la solución de los profesores (cambiar \"alta\" y \"baja\" por 0 y 1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f9c33c2a72f2e665"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
