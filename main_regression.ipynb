{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93068dce9f22b25b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Project Introduction:\n",
    "Primer laboratorio de aprendizaje automático, desarrollado por Sergio Barragán Blanco (100472343) y Eduardo Alarcón Navarro (100472175). \n",
    "Grupo 17.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933f5cb4b3926278",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 1. EDA\n",
    "Existen 22 carácterísticas que definen cada momento, de las cuales ninguna es categórica, todas son numéricas (con la energía suman 23). No existen valores faltantes, pero si los hubiera, los rellenaríamos con la media del valor superior e inferior antes de randomizar el dataset.\n",
    "\n",
    "No existen tampoco columnas constantes, que se eliminarían. \n",
    "\n",
    "Con todo esto, podemos observar que es un problema de regresión.\n",
    "\n",
    "La variable que estamos intentando predecir es la \"energía\" que es el valor de la energía generada 24 horas después. \n",
    "\n",
    "Por lo tanto, vamos a comenzar con todos los imports necesarios para este proyecto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e738704b6e206784",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "#import seaborn as sns\n",
    "import time\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, Normalizer, PowerTransformer\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, TimeSeriesSplit, cross_val_score\n",
    "from sklearn.tree import DecisionTreeRegressor  \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import SVR\n",
    "from joblib import dump, load\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312c5a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('wind_ava.csv.gz', compression='gzip')\n",
    "\n",
    "# FIlter the data to only include the columns that end in 13\n",
    "data = data.filter(regex='13$|energy')\n",
    "#print(data)\n",
    "print(data.head())\n",
    "print(\"--------------------\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d5e370",
   "metadata": {},
   "source": [
    "# Correlación:\n",
    "## Correlaciones entre parámetros:\n",
    "Al principio, parece que las columnas lai_lv.13 y lai_hv.13 tienen una correlación, pero según avanza el tiempo, desaparece.\n",
    "\n",
    "## Escala de los datos\n",
    "Entre las diferentes columnas de datos, tenemos valores y magnitudes muy dispares.\n",
    "\n",
    "Para comprobar la posible correlación entre datos, hemos usado la librería de matplotlib para hacer una matriz de correlación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b626d52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load Data, done in the first cell\n",
    "\n",
    "# Step 2: Calculate Correlation\n",
    "correlation_matrix = data.corr()\n",
    "\n",
    "# Step 3: Visualize Correlation Matrix\n",
    "plt.figure(figsize=(15, 12))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Matrix with all columns')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Step 4: Filter out columns with correlation more than 0.9\n",
    "# Al eliminar las columnas innecesarias, el modelo empeora en la predicción de la energía considerablemente.\n",
    "data_filtered = data.drop(columns=['lai_hv.13', 'u10.13', 'v10.13', 'stl3.13',\n",
    "                                    'iews.13', 'inss.13', 'u100.13', 'v100.13',\n",
    "                                      't2m.13', 'stl1.13', 'stl2.13'])\n",
    "print(data_filtered.head())\n",
    "\n",
    "# Repeat Step 3: Visualize Correlation Matrix\n",
    "correlation_matrix = data_filtered.corr()\n",
    "plt.figure(figsize=(15, 12))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Matrix with selected columns')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8686906c824078d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "A pesar de que la matriz de correlación sea mucho mejor, no se pueden sacar conclusiones claras de ella. Sobre todo, porque al entrenar los modelos, vemos que son peores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e8a83d",
   "metadata": {},
   "source": [
    "Depues de realizar las pruebas con los dos datasets, hemos visto que es mejor utilizar todos los datos,ya que los resultados so mucho mejores. Por lo que vamos a proceder a separar los datos de entrenamiento de los datos de entrenamiento y los datos de test, con train_test_split de la fuente de datos original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "97f5d899342eb290",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T15:12:54.718909300Z",
     "start_time": "2024-04-10T15:12:54.656493200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     p54.162.13  p55.162.13    cape.13    p59.162.13  lai_lv.13  lai_hv.13  \\\n",
      "0  2.510824e+06    9.186295  13.527577  1.386937e+06   2.344111   2.432983   \n",
      "1  2.513173e+06    8.849569   6.896412  1.153526e+06   2.343719   2.432838   \n",
      "2  2.509627e+06    7.924080   4.774439  1.098754e+06   2.343300   2.432704   \n",
      "3  2.510571e+06    6.922709   0.000000  1.076021e+06   2.342830   2.432514   \n",
      "4  2.505664e+06    6.646282   0.000000  1.070830e+06   2.342437   2.432369   \n",
      "\n",
      "    u10n.13   v10n.13         sp.13     stl1.13  ...      t2m.13     stl2.13  \\\n",
      "0 -0.757587 -1.922799  99846.319914  280.960661  ...  280.473098  281.042026   \n",
      "1 -1.412620 -1.403011  99917.733093  279.296651  ...  278.286616  280.747406   \n",
      "2 -2.290185 -0.754580  99764.378681  278.233956  ...  277.206490  280.114863   \n",
      "3 -3.497855  1.271028  99672.670459  280.787263  ...  280.926600  279.991138   \n",
      "4 -0.971249  0.553060  99372.811211  279.583112  ...  277.363875  280.576898   \n",
      "\n",
      "      stl3.13   iews.13   inss.13     stl4.13    fsr.13   flsr.13   u100.13  \\\n",
      "0  281.462478 -0.057958 -0.138650  284.684755  0.404731 -5.927092 -1.780562   \n",
      "1  281.486541 -0.103576 -0.083050  284.667948  0.404920 -5.913881 -3.743344   \n",
      "2  281.487095 -0.165721 -0.036241  284.651914  0.405704 -5.908272 -5.097203   \n",
      "3  281.472435 -0.275550  0.098192  284.636266  0.403967 -5.961995 -4.500835   \n",
      "4  281.473265 -0.056553  0.041844  284.620232  0.403808 -5.987860 -3.392324   \n",
      "\n",
      "    v100.13  \n",
      "0 -4.443617  \n",
      "1 -3.129469  \n",
      "2 -1.157748  \n",
      "3  1.502478  \n",
      "4  2.131114  \n",
      "\n",
      "[5 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "y = data['energy']\n",
    "X = data.drop(columns=['energy'])\n",
    "print(x.head())\n",
    "\n",
    "# test_size vamos a escoger 0.2 ya que tenemos datos en un rango de 5 años en orden (2015-2019) por lo que (asumiendo que el numero de datos para todos los años es similar) usaremos 2019 como test final.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "#print(f\"Fechas de tain {train.iloc[0].datetime}-{train-iloc[-1].datetime}\")\n",
    "\n",
    "graph_data = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0ecc92bc45e922",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "54e98abb555e4f64",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Decidir usando KNN - Scalers\n",
    "\n",
    "A continuación, usaremos KNN para determinar el método de escalado más apropiado para el problema. \n",
    "Dado que no queremos entrenar con datos en el futuro, no podremos usar una validacin cruzada normal (usando Kfold o Shuffle), por lo que tendremos que usar predefined split o Time Series split (el cual es igual a una validación cruzada, pero ignora los posteriores a test), en nuestro caso, usaremos TimeSeriesSplit para todas las validaciones. En cuanto a los scalers, probaremos \"StandardScaler\", \"MinMaxScaler\", \"RobustScaler\", \"Normalizer\" y \"PowerTransformer\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d318af744d9aa25a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scalers = [StandardScaler(), MinMaxScaler(), RobustScaler(), Normalizer(), PowerTransformer()]\n",
    "\n",
    "pipelines = {}\n",
    "for scaler in scalers:\n",
    "    pipe = Pipeline([\n",
    "        ('scaler', scaler),\n",
    "        ('knn', KNeighborsRegressor())\n",
    "    ])\n",
    "    pipelines[str(scaler)[:-2]] = pipe\n",
    "\n",
    "scores = {}\n",
    "\n",
    "for name, pipe in pipelines.items():\n",
    "    scores[name] = -cross_val_score(pipe, X_train, y_train, cv=TimeSeriesSplit(), scoring='neg_root_mean_squared_error').mean()\n",
    "\n",
    "\n",
    "for name, score in scores.items():\n",
    "    print (f\" {name}: {score}\")\n",
    "\n",
    "# Resultado con data sin filtrar\n",
    "# StandardScaler: 455.56751873979965\n",
    "# MinMaxScaler: 490.81263982750016\n",
    "# RobustScaler: 454.0893589365546\n",
    "# Normalizer: 644.0650360014222\n",
    "# PowerTransformer: 428.29574620911944\n",
    "\n",
    "# Resultado con data filtrada\n",
    "# StandardScaler: 518.1803408987827\n",
    "# MinMaxScaler: 538.7092117532035\n",
    "# RobustScaler: 525.9490003734232\n",
    "# Normalizer: 644.0650360014222\n",
    "# PowerTransformer: 497.1992872267256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcecc08219a0344",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Entrenamiento de diferentes modelos\n",
    "Lo próximo que haremos, será probar con diferentes métodos de entrenamiento (KNN, árboles de regresión, regresión lineal normal y la variante de Lasso, y SVM. Para elegir el mejor, usaremos la librería time para saber cuánto se tarda en entrenar cada modelo, un dummy para comparar resultados, y el mejor de entre ellos, usando una política de importancia de 20% a tiempo y 80% precisión.\n",
    "\n",
    "Lo primero que vamos a usar es el dummyRegressor, para tener una representación visual del peor escenario posible y que tanto mejoran nuestros diseños.\n",
    "\n",
    "Como el scaler que dió mejor scoring fué el PowerTransformer, será el que usaremos de ahora en adelante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f115f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Dummy regressor to obtain a baseline\n",
    "scaler = PowerTransformer()\n",
    "\n",
    "dummy = DummyRegressor(strategy='mean')\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('scaler', scaler),\n",
    "    ('model', dummy)\n",
    "])\n",
    "\n",
    "a = time.time()\n",
    "pipe.fit(X_train, y_train)\n",
    "b = time.time()\n",
    "y_pred = pipe.predict(X_test)\n",
    "\n",
    "mse_dummy = metrics.mean_squared_error(y_test, y_pred)\n",
    "rmse_dummy = np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
    "print(f\"MSE: {mse_dummy}\")\n",
    "print(f\"RMSE: {rmse_dummy}\")\n",
    "print(f\"Time taken to train the Dummy Model: {b-a} seconds\")\n",
    "\n",
    "graph_data[\"Dummy\"] = [rmse_dummy, b-a]\n",
    "# Resultado con data sin filtrar\n",
    "# MSE: 439516.0127766374\n",
    "# RMSE: 662.9600385970766\n",
    "# Time taken to train the Dummy Model: 0.0872499942779541\n",
    "\n",
    "# Resultado con data filtrada\n",
    "# MSE: 439516.0127766374\n",
    "# RMSE: 662.9600385970766\n",
    "# Time taken to train the Dummy Model: 0.02886199951171875 seconds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595af26d8e4e2799",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Para empezar, vamos a comparar con un **Decision Tree Regresor**. En vez de usar un Grid Search, usamos un RandomSearch porque es más rápido y para probar más combinaciones de hiperparámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f24c60f565a7dc",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# DecisionTreeRegressor\n",
    "\n",
    "param_grid = {\n",
    "    'criterion': ['friedman_mse', 'absolute_error', 'poisson', 'squared_error'],  # Adjust criterion for regression\n",
    "    'max_depth': [None, 10, 12, 13, 14, 15, 16, 17],\n",
    "    'min_samples_split': range(2, 15, 2),\n",
    "    'min_samples_leaf': range(10, 20, 2),\n",
    "}\n",
    "\n",
    "cv = TimeSeriesSplit()\n",
    "model = RandomizedSearchCV(DecisionTreeRegressor(random_state=7543),  # Use DecisionTreeRegressor\n",
    "                     param_grid, cv=cv, n_jobs=-1, verbose=1, scoring='neg_root_mean_squared_error',\n",
    "                     n_iter=50, random_state=4375)  # Adjust scoring metric\n",
    "\n",
    "model2 = GridSearchCV(DecisionTreeRegressor(random_state=7543),  # Use DecisionTreeRegressor\n",
    "                     param_grid, cv=cv, n_jobs=-1, verbose=1, scoring='neg_root_mean_squared_error')\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('model', model)\n",
    "])\n",
    "a = time.time()\n",
    "pipe.fit(X_train, y_train)\n",
    "b = time.time()\n",
    "y_pred = pipe.predict(X_test)\n",
    "\n",
    "best_params = pipe.named_steps['model'].best_params_\n",
    "score = pipe.named_steps['model'].best_score_\n",
    "print(f\"Best params: {best_params}\")\n",
    "print(f\"Best score: {-score}\")\n",
    "print(f\"Time taken to train the DecisionTreeRegressor Model: {b-a} seconds\")\n",
    "\n",
    "graph_data[\"Decision_Tree_R\"] = [-score, b-a]\n",
    "\n",
    "# Resultado con data sin filtrar\n",
    "# Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
    "# Best params: {'min_samples_split': 2, 'min_samples_leaf': 18, 'max_depth': 12, 'criterion': 'squared_error'}\n",
    "# Best score: 425.7363157160712\n",
    "# Time taken to train the DecisionTreeRegressor Model: 19.370752096176147\n",
    "\n",
    "# Resultado con data filtrada\n",
    "# Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
    "# Best params: {'min_samples_split': 8, 'min_samples_leaf': 16, 'max_depth': 15, 'criterion': 'poisson'}\n",
    "# Best score: 426.0749548459118\n",
    "# Time taken to train the DecisionTreeRegressor Model: 10.071922779083252 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba3270d69a5b0f4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Como hemos podido observar, el modelo es bastante mejor que el dummy, aproximadamente, un tercio mejor. No obstante probaremos con más modelos para contrastar.\n",
    "\n",
    "El próximo modelo es la **regresión lineal** normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'fit_intercept': [True, False],\n",
    "    'copy_X': [True, False],\n",
    "    'n_jobs': [-1],  # -1 indica utilizar todos los núcleos\n",
    "    'positive': [True, False]\n",
    "}\n",
    "\n",
    "\n",
    "# Crear el modelo de regresión lineal\n",
    "model = LinearRegression()\n",
    "cv = TimeSeriesSplit()\n",
    "\n",
    "grid_search_model = GridSearchCV(model,\n",
    "                           param_grid,\n",
    "                           cv=cv,\n",
    "                           scoring=\"neg_root_mean_squared_error\"\n",
    "                           )\n",
    "\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('scaler', scaler),\n",
    "    ('model', grid_search_model)\n",
    "])\n",
    "\n",
    "# Entrenar el modelo con los datos de entrenamiento\n",
    "a = time.time()\n",
    "pipe.fit(X_train, y_train)\n",
    "b = time.time()\n",
    "\n",
    "\n",
    "print(\"best model:\", pipe.named_steps['model'].best_estimator_)\n",
    "print(\"best params:\", pipe.named_steps['model'].best_params_)\n",
    "print(\"RMSE: \", -pipe.named_steps['model'].best_score_)\n",
    "print(f\"Time taken to train the SVR: {b-a} seconds\")\n",
    "\n",
    "graph_data[\"Linear_Regression\"] = [-pipe.named_steps['model'].best_score_, b-a]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "best model: LinearRegression(n_jobs=-1)\n",
    "best params: {'copy_X': True, 'fit_intercept': True, 'n_jobs': -1, 'positive': False}\n",
    "RMSE:  549.6089612265775\n",
    "Time taken to train the SVR: 0.3889331817626953 seconds\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "30b4efa4e3f5f415"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Como podemos observar, aunque es mejor que el dummy, está bastante peor que el árbol de decisión, no obstante, vamos a probar con  con la variante **Lasso** a ver si conseguimos mejorar el entrenamiento:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b4d0cc4ad036f267"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Definir los parámetros que deseas probar\n",
    "parameters = {\n",
    "    'alpha': [0.1, 0.5, 1.0, 2.0],  # Valores de regularización\n",
    "    'fit_intercept': [True, False],\n",
    "    'copy_X': [True, False], # pendientes de borrar\n",
    "    'positive': [True, False], \n",
    "    'precompute': [True, False], \n",
    "    'random_state': [7543], \n",
    "    'selection': ['cyclic', 'random'], \n",
    "    'tol': [1**(-10), 1**(-5), 1**(-4), 1**(-3), 1**(-2)], \n",
    "    'warm_start': [True, False]\n",
    "}\n",
    "\n",
    "# Crear el modelo de regresión lineal Lasso\n",
    "model = Lasso()\n",
    "cv = TimeSeriesSplit()\n",
    "grid_search = GridSearchCV(model, \n",
    "                           parameters, \n",
    "                           scoring='neg_root_mean_squared_error', \n",
    "                           cv=cv)\n",
    "a = time.time()\n",
    "grid_search.fit(X_train, y_train)\n",
    "b = time.time()\n",
    "\n",
    "\n",
    "print(\"best model:\", grid_search.best_estimator_)\n",
    "print(\"best params:\", grid_search.best_params_)\n",
    "print(\"RMSE: \", -grid_search.best_score_)\n",
    "print(f\"Time taken to train the SVR: {b-a} seconds\")\n",
    "\n",
    "graph_data[\"Lasso\"] = [-grid_search.best_score_, b-a]\n",
    "\n",
    "\"\"\"\n",
    "best model: Lasso(alpha=0.1, precompute=True, random_state=7543, selection='random',\n",
    "      tol=1.0, warm_start=True)\n",
    "best params: {'alpha': 0.1, 'copy_X': True, 'fit_intercept': True, 'positive': False, 'precompute': True, 'random_state': 7543, 'selection': 'random', 'tol': 1.0, 'warm_start': True}\n",
    "RMSE:  566.6685051282924\n",
    "Time taken to train the SVR: 27.60527014732361 seconds\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "434c96bff86cb11b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Desafortunadamente, el modelo es ligeramente peor que el de  regresión lineal normal, pero como los árboles le sacan bastante más precisión (aunque tardan más en entrenar), seguiremos decantándonos por ellos. \n",
    "\n",
    "No obstante, aun quedan algunos modelos por probar, por lo que el siguiente es el **KNN**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8bdf1e8e7131eb46"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3823beec798adab4",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Usaremos los parametros por default n_slits = 5, max_train_dize = None, test_size = None, gap = 0 \n",
    "cv = TimeSeriesSplit()\n",
    "\n",
    "param_grid = {\n",
    "    'n_neighbors': range(5, 40, 5),\n",
    "    'weights' : ['uniform', 'distance'],\n",
    "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "    'leaf_size': range(1, 25, 5),\n",
    "    'p' : [1,2], # distance `= 1 manhatan, p=2 euclidean\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(KNeighborsRegressor(),\n",
    "                     param_grid, cv=cv, n_jobs=-1, scoring='neg_root_mean_squared_error')\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('scaler', PowerTransformer()),\n",
    "    ('model', grid_search)\n",
    "])\n",
    "\n",
    "\n",
    "grid_search2 = GridSearchCV(KNeighborsRegressor(),\n",
    "                     param_grid, cv=cv, n_jobs=-1, scoring='neg_root_mean_squared_error')\n",
    "\n",
    "a = time.time()\n",
    "pipe.fit(X_train, y_train)\n",
    "b = time.time() \n",
    "grid_search2.fit(X_train, y_train)\n",
    "c = time.time()\n",
    "\n",
    "\n",
    "best_model = grid_search2.best_estimator_\n",
    "best_params = grid_search2.best_params_\n",
    "\n",
    "best_model_pipe = pipe.named_steps[\"model\"].best_estimator_\n",
    "best_params_pipe = pipe.named_steps[\"model\"].best_params_\n",
    "best_score_pipe = -pipe.named_steps[\"model\"].best_score_\n",
    "best_score = -grid_search2.best_score_\n",
    "\n",
    "print(\"Mejor modelo:\",best_model, \"<----------->\", best_model_pipe)\n",
    "print(\"Mejorres parámetros:\",best_params, \"<----------->\", best_params_pipe)\n",
    "print(\"Score:\", best_score , \"<----------->\", best_score_pipe)\n",
    "\n",
    "print(f\"Time taken to train the KNeighborsRegressor Model with scaler: {b-a} seconds\")\n",
    "print(f\"Time taken to train the KNeighborsRegressor Model without scaler: {c-b} seconds\")\n",
    "\n",
    "graph_data[\"KNN\"] = [best_score_pipe, b-a]\n",
    "\n",
    "\n",
    "# Resultado con data sin filtrar\n",
    "# Mejor modelo: KNeighborsRegressor(leaf_size=1, n_neighbors=35, p=1) <-----------> KNeighborsRegressor(leaf_size=1, n_neighbors=15, p=1, weights='distance')\n",
    "# Mejorres parámetros: {'algorithm': 'auto', 'leaf_size': 1, 'n_neighbors': 35, 'p': 1, 'weights': 'uniform'} <-----------> {'algorithm': 'auto', 'leaf_size': 1, 'n_neighbors': 15, 'p': 1, 'weights': 'distance'}\n",
    "# Score: 593.643591243938 <-----------> 412.17437045226825\n",
    "# Time taken to train the KNeighborsRegressor Model with scaler: 30.788731813430786 seconds\n",
    "# Time taken to train the KNeighborsRegressor Model without scaler: 12.302868366241455 seconds\n",
    "\n",
    "# Resultado con data filtrada\n",
    "# Mejor modelo: KNeighborsRegressor(leaf_size=1, n_neighbors=35, p=1) <-----------> KNeighborsRegressor(leaf_size=1, n_neighbors=15, p=1, weights='distance')\n",
    "# Mejorres parámetros: {'algorithm': 'auto', 'leaf_size': 1, 'n_neighbors': 35, 'p': 1, 'weights': 'uniform'} <-----------> {'algorithm': 'auto', 'leaf_size': 1, 'n_neighbors': 15, 'p': 1, 'weights': 'distance'}\n",
    "# Score: 593.643591243938 <-----------> 412.17437045226825\n",
    "# Time taken to train the KNeighborsRegressor Model with scaler: 12.555383205413818 seconds\n",
    "# Time taken to train the KNeighborsRegressor Model without scaler: 5.734422922134399 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bcdeee",
   "metadata": {},
   "source": [
    "Como hemos podido observar, el uso del scaler cambia totalmente el resultado obtenido, convirtiendose así en el mejor modelo hasta la fecha y el más probable para el entrenamiendo final, pero, antes de apresurarnos, probaremos con **SVR**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4ea393",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = TimeSeriesSplit()\n",
    "model = SVR()\n",
    "\n",
    "param_grid = {\n",
    "    'C': range(705, 720),\n",
    "    'epsilon': [0.1, 0.2, 0.3],\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "    'degree': [1, 2, 3, ],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(model,\n",
    "                            param_grid,\n",
    "                            cv=cv, \n",
    "                            n_jobs=-1, \n",
    "                            scoring='neg_root_mean_squared_error')\n",
    "\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('scaler', PowerTransformer()),\n",
    "    ('model', grid_search)\n",
    "])\n",
    "a = time.time()\n",
    "pipe.fit(X_train, y_train)\n",
    "b = time.time()\n",
    "\n",
    "\n",
    "print(np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "print(\"best model:\", pipe.named_steps['model'].best_estimator_)\n",
    "print(\"best params:\", pipe.named_steps['model'].best_params_)\n",
    "print(\"RMSE: \", -pipe.named_steps['model'].best_score_)\n",
    "print(f\"Time taken to train the SVR: {b-a} seconds\")\n",
    "\n",
    "graph_data[\"KNN\"] = [-pipe.named_steps['model'].best_score_, b-a]\n",
    "\n",
    "\n",
    "# time taken to run the code:\n",
    "# time = \"13m 46s\"\n",
    "# 384.2528063686719\n",
    "# best model: SVR(C=719, degree=1, gamma='auto')\n",
    "# best params: {'C': 719, 'degree': 1, 'epsilon': 0.1, 'gamma': 'auto', 'kernel': 'rbf'}\n",
    "# RMSE:  380.9578481516544\n",
    "\n",
    "# Resultado con data sin filtrar\n",
    "# 447.31691802522346\n",
    "# best model: SVR(C=719, degree=1, gamma='auto')\n",
    "# best params: {'C': 719, 'degree': 1, 'epsilon': 0.1, 'gamma': 'auto', 'kernel': 'rbf'}\n",
    "# RMSE:  380.9578440914255\n",
    "# Time taken to train the SVR: 266.7573239803314 seconds ~ 4.4 minutes\n",
    "\n",
    "# Resultado con data filtrada\n",
    "# 434.6028671711526\n",
    "# best model: SVR(C=719, degree=1, gamma='auto')\n",
    "# best params: {'C': 719, 'degree': 1, 'epsilon': 0.1, 'gamma': 'auto', 'kernel': 'rbf'}\n",
    "# RMSE:  380.9578440914255\n",
    "# Time taken to train the SVR: 284.6029191017151 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d630832432fb07e5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Y como podemos observar, los mejores modelos son SVR, KNN con la pipe creada y el Decision Tree regresor.\n",
    "\n",
    "A pesar de que el KNN se entrena en menos tiempo y tiene las segundas mejores métricas, usaremos el SVR para la versión final ya que consideramos que la diferencia de tiempo no es tan relevante como el beneficio que se obtiene. Y siguiendo las reglas previamente mencionadas, el tiempo nos importa un 20% y la precisión un 80%, por lo que \n",
    "(266)*0.2 + (380)*0.8 = 357.2\n",
    "(30)*0.2 + (412)*0.8 = 335.6"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Antes de entrenar el modelo final, vamos a hacer una comparación de todos los modelos hasta la fecha, tenemos dos diccionarios, una que va a estar \"hardcodeada\" para evitar tener que ejecutar todos los modelos, y otra que va a ser la que se guarde después de la ejecución de todos los modelos, usaremos las listas para mostrar una grafica comparando los tiempos y la precisión.\n",
    "\n",
    "Cabe destacar que las medidas de tiempo no son muy precisas ya que el número de hiperparámetros que probamos en diferentes modelos varía, y por lo tanto es normal que tarden más."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a6d00be549d3b27"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Datos de resultados\n",
    "results = {\"Dummy\": [663, 0.087], \"Decision_Tree_R\": [425, 19.37], \"Linear_Regression\": [549, 0.38], \"Lasso\": [566, 27.60], \"KNN\": [412, 30.78], \"SVR\": [380, 266.75]}\n",
    "\n",
    "#### Graficas generadas por ChatGPT ####\n",
    "\n",
    "# Extraer nombres de modelos y puntajes\n",
    "modelos = list(results.keys())\n",
    "scores = [score[0] for score in results.values()]\n",
    "tiempos = [tiempo[1] for tiempo in results.values()]\n",
    "\n",
    "# Crear el gráfico de barras\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Barra para los puntajes (RMSE)\n",
    "plt.barh(modelos, scores, color='skyblue', label='RMSE')\n",
    "\n",
    "# Barra para los tiempos de entrenamiento\n",
    "plt.barh(modelos, tiempos, color='salmon', alpha=0.7, label='Tiempo de entrenamiento (seg)')\n",
    "\n",
    "# Etiquetas y título\n",
    "plt.xlabel('Score / Tiempo de entrenamiento')\n",
    "plt.ylabel('Modelo')\n",
    "plt.title('Comparación de Puntajes y Tiempos de Entrenamiento')\n",
    "plt.legend()\n",
    "\n",
    "# Mostrar gráfico\n",
    "plt.gca().invert_yaxis()  # Invertir el eje y para mostrar el mejor modelo arriba\n",
    "plt.grid(axis='x')  # Añadir rejilla solo en el eje x para mayor claridad\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "56dbf77586bf501"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Y por último, vamos a preparar el modelo final para la competición:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "99e306ef70e427c6"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ea8d445276626183",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T15:13:07.035097500Z",
     "start_time": "2024-04-10T15:13:03.237293500Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrador\\OneDrive\\Escritorio\\Uni\\Tercero Uni\\Aprendizaje Automatico\\ML-82-17\\venv\\lib\\site-packages\\numpy\\core\\_methods.py:176: RuntimeWarning: overflow encountered in multiply\n",
      "  x = um.multiply(x, x, out=x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 384.2528063686719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrador\\OneDrive\\Escritorio\\Uni\\Tercero Uni\\Aprendizaje Automatico\\ML-82-17\\venv\\lib\\site-packages\\numpy\\core\\_methods.py:176: RuntimeWarning: overflow encountered in multiply\n",
      "  x = um.multiply(x, x, out=x)\n"
     ]
    },
    {
     "data": {
      "text/plain": "['best_model_svr.joblib']"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Al final el modelo seleccionado es el SVR, la variante para problemas de regresión de SVM, este tipo de modelos se basan en maquinas de soporte vectorial cuyo objetivo es minimizar la suma de la diferencia entre las predicciones y los valores reales.\n",
    "\"\"\"\n",
    "pipe = Pipeline([\n",
    "    ('scaler', PowerTransformer()),\n",
    "    ('model', SVR(C=719, degree=1, epsilon=0.1, gamma='auto', kernel='rbf'))\n",
    "])\n",
    "\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "y_pred_pipe = pipe.predict(X_test)\n",
    "\n",
    "rmse_pipe = np.sqrt(mean_squared_error(y_test, y_pred_pipe))\n",
    "print(\"RMSE:\", rmse_pipe)\n",
    "\n",
    "final_pipe = Pipeline([\n",
    "    ('scaler', PowerTransformer()),\n",
    "    ('model', SVR(C=719, degree=1, epsilon=0.1, gamma='auto', kernel='rbf'))\n",
    "])\n",
    "\n",
    "final_pipe.fit(X, y)\n",
    "\n",
    "# Save the model to a file\n",
    "dump(final_pipe, 'best_model_svr.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
