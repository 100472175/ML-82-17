{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93068dce9f22b25b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Project Introduction:\n",
    "Primer laboratorio de aprendizaje automático, desarrollado por Sergio Barragán Blanco (100472343) y Eduardo Alarcón Navarro (100472175). \n",
    "Grupo 17.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933f5cb4b3926278",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 1. EDA\n",
    "Existen 22 carácterísticas que definen cada momento, de las cuales ninguna es categórica, todas son numéricas (con la energía suman 23). No existen valores faltantes, pero si los hubiera, los rellenaríamos con la media del valor superior e inferior antes de randomizar el dataset.\n",
    "\n",
    "No existen tampoco columnas constantes, que se eliminarían. \n",
    "\n",
    "Con todo esto, podemos observar que es un problema de regresión.\n",
    "\n",
    "La variable que estamos intentando predecir es la \"energía\" que es el valor de la energía generada 24 horas después. \n",
    "\n",
    "Por lo tanto, vamos a comenzar con todos los imports necesarios para este proyecto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e738704b6e206784",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-20T08:33:55.511206800Z",
     "start_time": "2024-03-20T08:33:55.495556800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, Normalizer, PowerTransformer\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold, RandomizedSearchCV, TimeSeriesSplit, cross_val_score\n",
    "from sklearn.tree import DecisionTreeRegressor  \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8686906c824078d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "También, vamos a importar el documento con los datos de entrenamiento, y le vamos a eliminar las columnas innecesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718a0bf13cf1f98b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-20T08:34:00.910031900Z",
     "start_time": "2024-03-20T08:33:59.307157900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('wind_ava.csv.gz', compression='gzip')\n",
    "\n",
    "# FIlter the data to only include the columns that end in 13\n",
    "data = data.filter(regex='13$|energy')\n",
    "#print(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9742cebf2d115e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Y separar los datos de entrenamiento de los datos de test, con train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f5d899342eb290",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-20T08:34:02.286600400Z",
     "start_time": "2024-03-20T08:34:02.254805600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = data['energy']\n",
    "x = data.drop(columns=['energy'])\n",
    "print(x.head())\n",
    "\n",
    "# test_size vamos a escoger 0.2 ya que tenemos datos en un rango de 5 años en orden (2015-2019) por lo que (asumiendo que el numero de datos para todos los años es similar) usaremos 2019 como test final.\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, shuffle=False)\n",
    "#print(f\"Fechas de tain {train.iloc[0].datetime}-{train-iloc[-1].datetime}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0ecc92bc45e922",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "54e98abb555e4f64",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Decidir usando KNN - Scalers\n",
    "\n",
    "A continuación, usaremos KNN para determinar el método de escalado más apropiado para el problema. \n",
    "Dado que no queremos entrenar con datos en el futuro, no podremos usar una validacin cruzada normal (usando Kfold o Shuffle), por lo que tendremos que usar predefined split o Time Series split (el cual es igual a una validación cruzada, pero ignora los posteriores a test), en nuestro caso, usaremos TimeSeriesSplit para todas las validaciones. En cuanto a los scalers, probaremos \"StandardScaler\", \"MinMaxScaler\", \"RobustScaler\", \"Normalizer\" y \"PowerTransformer\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d318af744d9aa25a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-20T08:35:55.612546500Z",
     "start_time": "2024-03-20T08:35:54.241588700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scalers = [StandardScaler(), MinMaxScaler(), RobustScaler(), Normalizer(), PowerTransformer()]\n",
    "\n",
    "pipelines = {}\n",
    "for scaler in scalers:\n",
    "    pipe = Pipeline([\n",
    "        ('scaler', scaler),\n",
    "        ('knn', KNeighborsRegressor())\n",
    "    ])\n",
    "    pipelines[str(scaler)[:-2]] = pipe\n",
    "\n",
    "scores = {}\n",
    "\n",
    "for name, pipe in pipelines.items():\n",
    "    scores[name] = -cross_val_score(pipe, X_train, y_train, cv=TimeSeriesSplit(), scoring='neg_root_mean_squared_error').mean()\n",
    "\n",
    "\n",
    "for name, score in scores.items():\n",
    "    print (f\" {name}: {score}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcecc08219a0344",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Entrenamiento de diferentes modelos\n",
    "Lo próximo que haremos, será probar con diferentes métodos de entrenamiento (KNN, árboles de regresión, regresión lineal normal y la variante de Lasso, y SVM. Para elegir el mejor, usaremos la librería time para saber cuánto se tarda en entrenar cada modelo, un dummy para comparar resultados, y el mejor de entre ellos, usando una política de importancia de 20% a tiempo y 80% precisión.\n",
    "\n",
    "Lo primero que vamos a usar es el dummyRegressor, para tener una representación visual del peor escenario posible y que tanto mejoran nuestros diseños.\n",
    "\n",
    "Como el scaler que dió mejor scoring fué el PowerTransformer, será el que usaremos de ahora en adelante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f115f6a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-20T08:37:57.890453700Z",
     "start_time": "2024-03-20T08:37:57.851940700Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a Dummy regressor to obtain a baseline\n",
    "scaler = PowerTransformer()\n",
    "\n",
    "from sklearn.dummy import DummyRegressor\n",
    "dummy = DummyRegressor(strategy='mean')\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('scaler', scaler),\n",
    "    ('model', dummy)\n",
    "])\n",
    "\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "y_pred = pipe.predict(X_test)\n",
    "\n",
    "mse_dummy = metrics.mean_squared_error(y_test, y_pred)\n",
    "rmse_dummy = np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
    "print(f\"MSE: {mse_dummy}\")\n",
    "print(f\"RMSE: {rmse_dummy}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595af26d8e4e2799",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Para empezar, vamos a comparar con un Decision Tree Regresor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f24c60f565a7dc",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# DecisionTreeRegressor\n",
    "\n",
    "param_grid = {\n",
    "    'criterion': ['friedman_mse', 'absolute_error', 'poisson', 'squared_error'],  # Adjust criterion for regression\n",
    "    'max_depth': [None, 10, 12, 13, 14, 15, 16, 17],\n",
    "    'min_samples_split': range(2, 15, 2),\n",
    "    'min_samples_leaf': range(10, 20, 2),\n",
    "}\n",
    "\n",
    "cv = TimeSeriesSplit()\n",
    "model = RandomizedSearchCV(DecisionTreeRegressor(random_state=7543),  # Use DecisionTreeRegressor\n",
    "                     param_grid, cv=cv, n_jobs=-1, verbose=1, scoring='neg_root_mean_squared_error', n_iter=50)  # Adjust scoring metric\n",
    "\n",
    "model2 = GridSearchCV(DecisionTreeRegressor(random_state=7543),  # Use DecisionTreeRegressor\n",
    "                     param_grid, cv=cv, n_jobs=-1, verbose=1, scoring='neg_root_mean_squared_error')\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('model', model)\n",
    "])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "y_pred = pipe.predict(X_test)\n",
    "\n",
    "best_params = pipe.named_steps['model'].best_params_\n",
    "score = pipe.named_steps['model'].best_score_\n",
    "print(f\"Best params: {best_params}\")\n",
    "print(f\"Best score: {-score}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba3270d69a5b0f4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Como hemos podido observar, el modelo es bastante mejor que el dummy, aproximadamente, un tercio mejor. No obstante probaremos con más modelos para contrastar.\n",
    "\n",
    "El próximo modelo es la **regresión lineal** normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f275a987e9421aae",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# LinearRegression\n",
    "param_grid = {\n",
    "    'fit_intercept': [True, False],\n",
    "    'copy_X': [True, False],\n",
    "    'n_jobs': [-1],  # -1 indica utilizar todos los núcleos\n",
    "    'positive': [True, False]\n",
    "}\n",
    "\n",
    "\n",
    "# Crear el modelo de regresión lineal\n",
    "model = LinearRegression()\n",
    "cv = TimeSeriesSplit()\n",
    "\n",
    "grid_search_model = GridSearchCV(model,\n",
    "                           param_grid,\n",
    "                           cv=cv,\n",
    "                           scoring=\"neg_root_mean_squared_error\"\n",
    "                           )\n",
    "\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('scaler', scaler),\n",
    "    ('model', grid_search_model)\n",
    "])\n",
    "\n",
    "# Entrenar el modelo con los datos de entrenamiento\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "best_params = pipe.named_steps['model'].best_params_\n",
    "score = pipe.named_steps['model'].best_score_\n",
    "print(f\"Best params: {best_params}\")\n",
    "print(f\"Best score: {-score}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3135ff92e842b77",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Como podemos observar, aunque es mejor que el dummy, está bastante peor que el árbol de decisión, no obstante, vamos a probar con  con la variante de Lasso a ver si conseguimos mejorar el entrenamiento:\n",
    "\n",
    "Con el modelo Lasso, no hemos utilizado un Scaler porque hemos visto que empeoraba el resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc265db12c228b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Lasso\n",
    "\n",
    "# Definir los parámetros que deseas probar\n",
    "parameters = {\n",
    "    'alpha': [0.1, 0.5, 1.0, 2.0],  # Valores de regularización\n",
    "    'fit_intercept': [True, False],\n",
    "    'copy_X': [True, False], # pendientes de borrar\n",
    "    'positive': [True, False], \n",
    "    'precompute': [True, False], \n",
    "    'random_state': [7543], \n",
    "    'selection': ['cyclic', 'random'], \n",
    "    'tol': [1**(-10), 1**(-5), 1**(-4), 1**(-3), 1**(-2)], \n",
    "    'warm_start': [True, False]\n",
    "}\n",
    "\n",
    "# Crear el modelo de regresión lineal Lasso\n",
    "model = Lasso()\n",
    "cv = TimeSeriesSplit()\n",
    "grid_search = GridSearchCV(model, \n",
    "                           parameters, \n",
    "                           scoring='neg_root_mean_squared_error', \n",
    "                           cv=cv)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Obtener el mejor modelo y sus hiperparámetros\n",
    "best_model = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "best_score = -grid_search.best_score_\n",
    "\n",
    "print(\"Mejor modelo:\",best_model)\n",
    "print(\"Mejorres parámetros:\",best_params)\n",
    "print(\"Score: \", best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609f3ff6c09b9bfc",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Afortunadamente, el modelo es mejor que la regresión lineal normal, pero los árboles le sacan bastante más precisión (aunque tardan más en entrenar, el beneficio sobrepasa los costes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3823beec798adab4",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# Usaremos los parametros por default n_slits = 5, max_train_dize = None, test_size = None, gap = 0 \n",
    "cv = TimeSeriesSplit()\n",
    "\n",
    "param_grid = {\n",
    "    'n_neighbors': range(5, 40, 5),\n",
    "    'weights' : ['uniform', 'distance'],\n",
    "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "    'leaf_size': range(1, 25, 5),\n",
    "    'p' : [1,2], # distance `= 1 manhatan, p=2 euclidean\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(KNeighborsRegressor(),\n",
    "                     param_grid, cv=cv, n_jobs=-1, scoring='neg_root_mean_squared_error')\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('scaler', PowerTransformer()),\n",
    "    ('model', grid_search)\n",
    "])\n",
    "\n",
    "\n",
    "grid_search2 = GridSearchCV(KNeighborsRegressor(),\n",
    "                     param_grid, cv=cv, n_jobs=-1, scoring='neg_root_mean_squared_error')\n",
    "\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "grid_search2.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "best_model = grid_search2.best_estimator_\n",
    "best_params = grid_search2.best_params_\n",
    "\n",
    "best_model_pipe = pipe.named_steps[\"model\"].best_estimator_\n",
    "best_params_pipe = pipe.named_steps[\"model\"].best_params_\n",
    "best_score_pipe = -pipe.named_steps[\"model\"].best_score_\n",
    "best_score = -grid_search2.best_score_\n",
    "\n",
    "print(\"Mejor modelo:\",best_model, \"<----------->\", best_model_pipe)\n",
    "print(\"Mejorres parámetros:\",best_params, \"<----------->\", best_params_pipe)\n",
    "print(\"Score:\", best_score , \"<----------->\", best_score_pipe)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bcdeee",
   "metadata": {},
   "source": [
    "We will now use the SVM to train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4ea393",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = TimeSeriesSplit()\n",
    "model = SVR()\n",
    "\n",
    "param_grid = {\n",
    "    'C': range(705, 720),\n",
    "    'epsilon': [0.1, 0.2, 0.3],\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "    'degree': [1, 2, 3, ],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(model,\n",
    "                            param_grid,\n",
    "                            cv=cv, \n",
    "                            n_jobs=-1, \n",
    "                            scoring='neg_root_mean_squared_error')\n",
    "\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('scaler', PowerTransformer()),\n",
    "    ('model', grid_search)\n",
    "])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print(np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "print(\"best model:\", pipe.named_steps['model'].best_estimator_)\n",
    "print(\"best params:\", pipe.named_steps['model'].best_params_)\n",
    "print(\"RMSE: \", -pipe.named_steps['model'].best_score_)\n",
    "\n",
    "# time taken to run the code:\n",
    "# time = \"13m 46s\"\n",
    "# 384.2528063686719\n",
    "# best model: SVR(C=719, degree=1, gamma='auto')\n",
    "# best params: {'C': 719, 'degree': 1, 'epsilon': 0.1, 'gamma': 'auto', 'kernel': 'rbf'}\n",
    "# RMSE:  380.9578481516544"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d630832432fb07e5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Y como podemos observar, los mejores modelos son KNN con la pipe creada y el Decision Tree regresor.\n",
    "\n",
    "Dado que el KNN se entrena en menos tiempo y tiene mejores métricas, será el usado para la versión final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8d445276626183",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "pipe = Pipeline([\n",
    "    ('scaler', PowerTransformer()),\n",
    "    ('model', SVR(C=719, degree=1, epsilon=0.1, gamma='auto', kernel='rbf'))\n",
    "])\n",
    "\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "y_pred_pipe = pipe.predict(X_test)\n",
    "\n",
    "rmse_pipe = np.sqrt(mean_squared_error(y_test, y_pred_pipe))\n",
    "print(\"RMSE:\", rmse_pipe)\n",
    "\n",
    "from joblib import dump, load\n",
    "dump(pipe, 'best_model_svr.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25629a1e",
   "metadata": {},
   "source": [
    "Ahora vamos a coger todos los datos que tenemos para entrenar un modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574d85b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('scaler', PowerTransformer()),\n",
    "    ('model', SVR(C=719, degree=1, epsilon=0.1, gamma='auto', kernel='rbf'))\n",
    "])\n",
    "\n",
    "pipe.fit(x, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1707edb484fdf4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Correlaciones entre parámetros:\n",
    "Al principio, parece que las columnas lai_lv.13 y lai_hv.13 tienen una correlación, pero según avanza el tiempo, desaparece.\n",
    "\n",
    "## Escala de los datos\n",
    "Entre las diferentes columnas de datos, tenemos valores y magnitudes muy dispares."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
