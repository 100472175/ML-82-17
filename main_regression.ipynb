{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93068dce9f22b25b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Project Introduction:\n",
    "Primer laboratorio de aprendizaje automático, desarrollado por Sergio Barragán Blanco (100472343) y Eduardo Alarcón Navarro (100472175). \n",
    "Grupo 17.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933f5cb4b3926278",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 1. EDA\n",
    "Existen 22 carácterísticas que definen cada momento, de las cuales ninguna es categórica, todas son numéricas (con la energía suman 23). No existen valores faltantes, pero si los hubiera, los rellenaríamos con la media del valor superior e inferior.\n",
    "\n",
    "No existen tampoco columnas constantes, que se eliminarían. \n",
    "\n",
    "Con todo esto, podemos observar que es un problema de regresión.\n",
    "\n",
    "La variable que estamos intentando predecir es la \"energía\" que es el valor de la energía generada 24 horas después. \n",
    "\n",
    "Por lo tanto, vamos a comenzar con todos los imports necesarios para este proyecto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e738704b6e206784",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import time\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, Normalizer, PowerTransformer\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, TimeSeriesSplit, cross_val_score\n",
    "from sklearn.tree import DecisionTreeRegressor  \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import SVR\n",
    "from joblib import dump, load\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Posteriormente cargamos el dataset de wind_ava.csv, y eliminamos todas las columnas a excepción de la energía y los datos que usaremos para predecirla (13)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1c8879d209a2b20a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312c5a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('wind_ava.csv.gz', compression='gzip')\n",
    "\n",
    "# FIlter the data to only include the columns that end in 13\n",
    "data = data.filter(regex='13$|energy')\n",
    "#print(data)\n",
    "print(data.head())\n",
    "print(\"--------------------\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d5e370",
   "metadata": {},
   "source": [
    "# Correlación:\n",
    "## Correlaciones entre parámetros:\n",
    "Al principio, parece que las columnas lai_lv.13 y lai_hv.13 tienen una correlación, pero según avanza el tiempo, desaparece.\n",
    "\n",
    "## Escala de los datos\n",
    "Entre las diferentes columnas de datos, tenemos valores y magnitudes muy dispares.\n",
    "\n",
    "Para comprobar la posible correlación entre datos, hemos usado la librería de matplotlib para hacer una matriz de correlación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b626d52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load Data, done in the first cell\n",
    "\n",
    "# Step 2: Calculate Correlation\n",
    "correlation_matrix = data.corr()\n",
    "\n",
    "# Step 3: Visualize Correlation Matrix\n",
    "plt.figure(figsize=(15, 12))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Matrix with all columns')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Step 4: Filter out columns with correlation more than 0.9\n",
    "# Al eliminar las columnas innecesarias, el modelo empeora en la predicción de la energía considerablemente.\n",
    "data_filtered = data.drop(columns=['lai_hv.13', 'u10.13', 'v10.13', 'stl3.13',\n",
    "                                    'iews.13', 'inss.13', 'u100.13', 'v100.13',\n",
    "                                      't2m.13', 'stl1.13', 'stl2.13'])\n",
    "print(data_filtered.head())\n",
    "\n",
    "# Repeat Step 3: Visualize Correlation Matrix\n",
    "correlation_matrix = data_filtered.corr()\n",
    "plt.figure(figsize=(15, 12))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Matrix with selected columns')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8686906c824078d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "A pesar de que la matriz de correlación sea mucho mejor, no se pueden sacar conclusiones claras de ella. Sobre todo, porque al entrenar los modelos, vemos que son peores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e8a83d",
   "metadata": {},
   "source": [
    "Depues de realizar las pruebas con los dos datasets, hemos visto que es mejor utilizar todos los datos,ya que los resultados so mucho mejores. Por lo que vamos a proceder a separar los datos de entrenamiento de los datos de entrenamiento y los datos de test, con train_test_split de la fuente de datos original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f5d899342eb290",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = data['energy']\n",
    "X = data.drop(columns=['energy'])\n",
    "print(X.head())\n",
    "\n",
    "# test_size vamos a escoger 0.2 ya que tenemos datos en un rango de 5 años en orden (2015-2019) por lo que (asumiendo que el numero de datos para todos los años es similar) usaremos 2019 como test final.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "#print(f\"Fechas de tain {train.iloc[0].datetime}-{train-iloc[-1].datetime}\")\n",
    "\n",
    "graph_data = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0ecc92bc45e922",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Comportamiento futuro\n",
    "Para este ejercicio vamos a llevar a cabo evaluaciones inner de varios tipos de modelos, los cuales poseerán distintos hyperparámetros que iremos ajustando. Posteriormente, compararemos los modelos y haremos una gráfica para representar visualmente las diferencias, y al mejor modelo lo seleccionaremos para probarlo con el conjunto de datos de test. Por último, entrenaremos ese modelo nuevamente, pero en este caso con todos los datos (incluidos el test) para poder mejorar (aunque sea ligeramente), la predicción a futuro del wind_comp.csv\n",
    "\n",
    "En cuanto a las métricas que se van a usar, la principal será la Root Mean Squeared Error (RMSE), ya que es una forma compacta de evaluar los modelos, además penaliza gravemente los fallos, y es mas legible que el Mean Squared Error (MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e98abb555e4f64",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Decidir usando KNN - Scalers\n",
    "\n",
    "A continuación, usaremos KNN para determinar el método de escalado más apropiado para el problema. \n",
    "Dado que no queremos entrenar con datos en el futuro, no podremos usar una validacin cruzada normal (usando Kfold o Shuffle), por lo que tendremos que usar predefined split o Time Series split (el cual es igual a una validación cruzada, pero ignora los posteriores a test), en nuestro caso, usaremos TimeSeriesSplit para todas las validaciones. En cuanto a los scalers, probaremos \"StandardScaler\", \"MinMaxScaler\", \"RobustScaler\", \"Normalizer\" y \"PowerTransformer\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d318af744d9aa25a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First of all, the necessary scalers\n",
    "scalers = [StandardScaler(), MinMaxScaler(), RobustScaler(), Normalizer(), PowerTransformer()]\n",
    "\n",
    "# And we are going to store all the pipelines in order to test them later\n",
    "pipelines = {}\n",
    "for scaler in scalers:\n",
    "    pipe = Pipeline([\n",
    "        ('scaler', scaler),\n",
    "        ('knn', KNeighborsRegressor())\n",
    "    ])\n",
    "    pipelines[str(scaler)[:-2]] = pipe\n",
    "\n",
    "scores = {}\n",
    "# Calculate the score for each scaler\n",
    "for name, pipe in pipelines.items():\n",
    "    scores[name] = -cross_val_score(pipe, X_train, y_train, cv=TimeSeriesSplit(), scoring='neg_root_mean_squared_error').mean()\n",
    "\n",
    "# Print them all and compare which one is the best\n",
    "for name, score in scores.items():\n",
    "    print (f\" {name}: {score}\")\n",
    "\n",
    "# Resultado con data sin filtrar\n",
    "# StandardScaler: 455.56751873979965\n",
    "# MinMaxScaler: 490.81263982750016\n",
    "# RobustScaler: 454.0893589365546\n",
    "# Normalizer: 644.0650360014222\n",
    "# PowerTransformer: 428.29574620911944\n",
    "\n",
    "# Resultado con data filtrada\n",
    "# StandardScaler: 518.1803408987827\n",
    "# MinMaxScaler: 538.7092117532035\n",
    "# RobustScaler: 525.9490003734232\n",
    "# Normalizer: 644.0650360014222\n",
    "# PowerTransformer: 497.1992872267256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcecc08219a0344",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Entrenamiento de diferentes modelos\n",
    "Lo próximo que haremos, será probar con diferentes métodos de entrenamiento (KNN, árboles de regresión, regresión lineal normal y la variante de Lasso, y SVM. Para elegir el mejor, usaremos la librería time para saber cuánto se tarda en entrenar cada modelo, un dummy para comparar resultados, y el mejor de entre ellos, usando una política de importancia de 20% a tiempo y 80% precisión.\n",
    "\n",
    "Lo primero que vamos a usar es el dummyRegressor, para tener una representación visual del peor escenario posible y que tanto mejoran nuestros diseños.\n",
    "\n",
    "Como el scaler que dió mejor scoring fué el PowerTransformer, será el que usaremos de ahora en adelante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f115f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Dummy regressor to obtain a baseline\n",
    "scaler = PowerTransformer()\n",
    "\n",
    "dummy = DummyRegressor(strategy='mean')\n",
    "# We will use the previously mentioned scaler\n",
    "pipe = Pipeline([\n",
    "    ('scaler', scaler),\n",
    "    ('model', dummy)\n",
    "])\n",
    "# Store the time so we can use it to compare every model\n",
    "a = time.time()\n",
    "# Train the model\n",
    "pipe.fit(X_train, y_train)\n",
    "b = time.time()\n",
    "# We will use the training test to have a reference for the other models\n",
    "y_pred = pipe.predict(X_test)\n",
    "\n",
    "# But the later models will only be using the score with RMSE\n",
    "mse_dummy = metrics.mean_squared_error(y_test, y_pred)\n",
    "rmse_dummy = np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(f\"MSE: {mse_dummy}\")\n",
    "print(f\"RMSE: {rmse_dummy}\")\n",
    "print(f\"Time taken to train the Dummy Model: {b-a} seconds\")\n",
    "\n",
    "graph_data[\"Dummy\"] = [rmse_dummy, b-a]\n",
    "# Resultado con data sin filtrar\n",
    "# MSE: 439516.0127766374\n",
    "# RMSE: 662.9600385970766\n",
    "# Time taken to train the Dummy Model: 0.0872499942779541\n",
    "\n",
    "# Resultado con data filtrada\n",
    "# MSE: 439516.0127766374\n",
    "# RMSE: 662.9600385970766\n",
    "# Time taken to train the Dummy Model: 0.02886199951171875 seconds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595af26d8e4e2799",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Para empezar, vamos a comparar con un **Decision Tree Regresor**. En vez de usar un Grid Search, usamos un RandomSearch porque es más rápido y para probar más combinaciones de hiperparámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f24c60f565a7dc",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# DecisionTreeRegressor\n",
    "# First of all is defining the parameters that will be used to train the model\n",
    "param_grid = {\n",
    "    'criterion': ['friedman_mse', 'absolute_error', 'poisson', 'squared_error'],  # Impurity criteria for regression\n",
    "    'max_depth': [None, 10, 12, 13, 14, 15, 16, 17],  # Maximum depth of the tree\n",
    "    'min_samples_split': range(2, 15, 2),  # Minimum number of samples to split an internal node\n",
    "    'min_samples_leaf': range(10, 20, 2),  # Minimum number of samples in a leaf node\n",
    "}\n",
    "# Then, since we are doing a inner evaluation, we will use TimeSeriesSplit() in order to keep the chronological order of the data\n",
    "cv = TimeSeriesSplit()\n",
    "model = RandomizedSearchCV(DecisionTreeRegressor(),  \n",
    "                     param_grid, cv=cv, n_jobs=-1, verbose=1, scoring='neg_root_mean_squared_error',\n",
    "                     n_iter=50, random_state=4375) \n",
    "# We will also train the model without a scaler\n",
    "model2 = GridSearchCV(DecisionTreeRegressor(),  \n",
    "                     param_grid, cv=cv, n_jobs=-1, verbose=1, scoring='neg_root_mean_squared_error')\n",
    "# Model without HPO\n",
    "model_whpo = GridSearchCV(DecisionTreeRegressor(),\n",
    "                          {}, cv=cv, n_jobs=-1, verbose=1, scoring='neg_root_mean_squared_error')\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('scaler', PowerTransformer()),\n",
    "    ('model', model)\n",
    "])\n",
    "\n",
    "pipe_whpo = Pipeline([\n",
    "    ('scaler', PowerTransformer()),\n",
    "    ('model', model_whpo)\n",
    "])\n",
    "\n",
    "a = time.time()\n",
    "pipe.fit(X_train, y_train)\n",
    "b = time.time()\n",
    "model2.fit(X_train, y_train)\n",
    "c = time.time()\n",
    "pipe_whpo.fit(X_train, y_train)\n",
    "d = time.time()\n",
    "\n",
    "best_params = pipe.named_steps['model'].best_params_\n",
    "score = pipe.named_steps['model'].best_score_\n",
    "\n",
    "best_params_whpo = pipe_whpo.named_steps['model'].best_params_\n",
    "score_whpo = pipe_whpo.named_steps['model'].best_score_\n",
    "\n",
    "print(\"With scaler <-----> Without scaler <-----> Without HPO\")\n",
    "print(f\"Best params: {best_params} <------> {model2.best_params_} <------> {best_params_whpo}\")\n",
    "print(f\"Best score: {-score} <------> {model2.best_score_} <------> {-score_whpo}\")\n",
    "print(f\"Time taken to train the DecisionTreeRegressor Model: {b-a} seconds <-----> {c-b} seconds <------> {d-c} seconds\")\n",
    "\n",
    "graph_data[\"Decision_Tree_R\"] = [-score, b-a]\n",
    "\n",
    "# Resultado con data sin filtrar\n",
    "# Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
    "# Best params: {'min_samples_split': 2, 'min_samples_leaf': 18, 'max_depth': 12, 'criterion': 'squared_error'}\n",
    "# Best score: 425.7363157160712\n",
    "# Time taken to train the DecisionTreeRegressor Model: 19.370752096176147\n",
    "\n",
    "# Resultado con data filtrada\n",
    "# Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
    "# Best params: {'min_samples_split': 8, 'min_samples_leaf': 16, 'max_depth': 15, 'criterion': 'poisson'}\n",
    "# Best score: 426.0749548459118\n",
    "# Time taken to train the DecisionTreeRegressor Model: 10.071922779083252 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba3270d69a5b0f4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Como hemos podido observar, el modelo es bastante mejor que el dummy, aproximadamente, un tercio mejor. No obstante probaremos con más modelos para contrastar.\n",
    "\n",
    "El próximo modelo es la **regresión lineal** normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'fit_intercept': [True, False],  # Whether to calculate the intercept for this model\n",
    "    'copy_X': [True, False],  # Whether to make a copy of X or overwrite it\n",
    "    'n_jobs': [-1],  # Number of jobs to run in parallel, -1 indicates using all processors\n",
    "    'positive': [True, False]  # When set to True, forces the coefficients to be positive\n",
    "}\n",
    "\n",
    "# Crear el modelo de regresión lineal\n",
    "cv = TimeSeriesSplit()\n",
    "\n",
    "grid_search_model = GridSearchCV(LinearRegression(),\n",
    "                           param_grid,\n",
    "                           cv=cv,\n",
    "                           scoring=\"neg_root_mean_squared_error\"\n",
    "                           )\n",
    "without_hpo = GridSearchCV(LinearRegression(),\n",
    "                           {},\n",
    "                           cv=cv,\n",
    "                           scoring=\"neg_root_mean_squared_error\"\n",
    "                           )\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('scaler', scaler),\n",
    "    ('model', grid_search_model)\n",
    "])\n",
    "\n",
    "pipe_whpo = Pipeline([\n",
    "    ('scaler', PowerTransformer()),\n",
    "    ('model', without_hpo)\n",
    "])\n",
    "\n",
    "# Entrenar el modelo con los datos de entrenamiento\n",
    "a = time.time()\n",
    "pipe.fit(X_train, y_train)\n",
    "b = time.time()\n",
    "pipe_whpo.fit(X_train, y_train)\n",
    "c = time.time()\n",
    "\n",
    "best_params = pipe.named_steps['model'].best_params_\n",
    "score = pipe.named_steps['model'].best_score_\n",
    "\n",
    "best_params_whpo = pipe_whpo.named_steps['model'].best_params_\n",
    "score_whpo = pipe_whpo.named_steps['model'].best_score_\n",
    "\n",
    "print(\"With scaler <-----> Without HPO\")\n",
    "print(f\"Best params: {best_params} <------> {best_params_whpo}\")\n",
    "print(f\"Best score: {-score} <------> {-score_whpo}\")\n",
    "print(f\"Time taken to train the DecisionTreeRegressor Model: {b-a} seconds <-----> {c-b} seconds\")\n",
    "\n",
    "graph_data[\"Linear_Regression\"] = [-pipe.named_steps['model'].best_score_, b-a]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "best model: LinearRegression(n_jobs=-1)\n",
    "best params: {'copy_X': True, 'fit_intercept': True, 'n_jobs': -1, 'positive': False}\n",
    "RMSE:  549.6089612265775\n",
    "Time taken to train the SVR: 0.3889331817626953 seconds\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "30b4efa4e3f5f415"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Como podemos observar, aunque es mejor que el dummy, está bastante peor que el árbol de decisión, no obstante, vamos a probar con  con la variante **Lasso** a ver si conseguimos mejorar el entrenamiento:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b4d0cc4ad036f267"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Definir los parámetros que deseas probar\n",
    "parameters = {\n",
    "    'alpha': [0.1, 0.5, 1.0, 2.0],  # Regularization parameter\n",
    "    'fit_intercept': [True, False],  # Calculate the intercept or not\n",
    "    'copy_X': [True, False],  \n",
    "    'positive': [True, False],  # When set to True, forces the coefficients to be positive\n",
    "    'precompute': [True, False],  # Compute the Gram matrix or not\n",
    "    'selection': ['cyclic', 'random'],  # Ways of fitting the model\n",
    "    'tol': [1**(-10), 1**(-5), 1**(-4), 1**(-3), 1**(-2)],  # Tolerance for stopping criteria\n",
    "    'warm_start': [True, False]  \n",
    "}\n",
    "\n",
    "\n",
    "# Crear el modelo de regresión lineal Lasso\n",
    "cv = TimeSeriesSplit()\n",
    "grid_search = GridSearchCV(Lasso(), \n",
    "                           parameters, \n",
    "                           scoring='neg_root_mean_squared_error', \n",
    "                           cv=cv)\n",
    "\n",
    "without_hpo = GridSearchCV(Lasso(),\n",
    "                           {},\n",
    "                           cv=cv,\n",
    "                           scoring=\"neg_root_mean_squared_error\"\n",
    "                           )\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('scaler', scaler),\n",
    "    ('model', grid_search_model)\n",
    "])\n",
    "\n",
    "pipe_whpo = Pipeline([\n",
    "    ('scaler', PowerTransformer()),\n",
    "    ('model', without_hpo)\n",
    "])\n",
    "\n",
    "# Entrenar el modelo con los datos de entrenamiento\n",
    "a = time.time()\n",
    "pipe.fit(X_train, y_train)\n",
    "b = time.time()\n",
    "pipe_whpo.fit(X_train, y_train)\n",
    "c = time.time()\n",
    "\n",
    "best_params = pipe.named_steps['model'].best_params_\n",
    "score = pipe.named_steps['model'].best_score_\n",
    "\n",
    "best_params_whpo = pipe_whpo.named_steps['model'].best_params_\n",
    "score_whpo = pipe_whpo.named_steps['model'].best_score_\n",
    "\n",
    "print(\"With scaler <-----> Without HPO\")\n",
    "print(f\"Best params: {best_params} <------> {best_params_whpo}\")\n",
    "print(f\"Best score: {-score} <------> {-score_whpo}\")\n",
    "print(f\"Time taken to train the DecisionTreeRegressor Model: {b-a} seconds <-----> {c-b} seconds\")\n",
    "\n",
    "\n",
    "graph_data[\"Lasso\"] = [-grid_search.best_score_, b-a]\n",
    "\n",
    "\"\"\"\n",
    "best model: Lasso(alpha=0.1, precompute=True, random_state=7543, selection='random',\n",
    "      tol=1.0, warm_start=True)\n",
    "best params: {'alpha': 0.1, 'copy_X': True, 'fit_intercept': True, 'positive': False, 'precompute': True, 'random_state': 7543, 'selection': 'random', 'tol': 1.0, 'warm_start': True}\n",
    "RMSE:  566.6685051282924\n",
    "Time taken to train the SVR: 27.60527014732361 seconds\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "434c96bff86cb11b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Desafortunadamente, el modelo es ligeramente peor que el de  regresión lineal normal, pero como los árboles le sacan bastante más precisión (aunque tardan más en entrenar), seguiremos decantándonos por ellos. \n",
    "\n",
    "No obstante, aun quedan algunos modelos por probar, por lo que el siguiente es el **KNN**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8bdf1e8e7131eb46"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3823beec798adab4",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Usaremos los parametros por default n_slits = 5, max_train_dize = None, test_size = None, gap = 0 \n",
    "cv = TimeSeriesSplit()\n",
    "\n",
    "param_grid = {\n",
    "    'n_neighbors': range(5, 40, 5),  # Number of neighbors to consider\n",
    "    'weights' : ['uniform', 'distance'],  # Weight function used in prediction\n",
    "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],  # Algorithm used to compute nearest neighbors\n",
    "    'leaf_size': range(1, 25, 5),  # Leaf size passed to BallTree or KDTree\n",
    "    'p' : [1,2],  # Distance metric (1 for Manhattan, 2 for Euclidean)\n",
    "}\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(KNeighborsRegressor(),\n",
    "                     param_grid, cv=cv, n_jobs=-1, scoring='neg_root_mean_squared_error')\n",
    "\n",
    "without_hpo = GridSearchCV(KNeighborsRegressor(),\n",
    "                           {}, cv=cv, n_jobs=-1, scoring='neg_root_mean_squared_error')\n",
    "\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('scaler', PowerTransformer()),\n",
    "    ('model', grid_search)\n",
    "])\n",
    "\n",
    "pipe_whpo = Pipeline([\n",
    "    ('scaler', PowerTransformer()),\n",
    "    ('model', without_hpo)\n",
    "])\n",
    "\n",
    "\n",
    "grid_search2 = GridSearchCV(KNeighborsRegressor(),\n",
    "                     param_grid, cv=cv, n_jobs=-1, scoring='neg_root_mean_squared_error')\n",
    "\n",
    "a = time.time()\n",
    "pipe.fit(X_train, y_train)\n",
    "b = time.time() \n",
    "grid_search2.fit(X_train, y_train)\n",
    "c = time.time()\n",
    "pipe_whpo.fit(X_train, y_train)\n",
    "d = time.time()\n",
    "\n",
    "best_model = grid_search2.best_estimator_\n",
    "best_params = grid_search2.best_params_\n",
    "best_score = -grid_search2.best_score_\n",
    "\n",
    "best_model_pipe = pipe.named_steps[\"model\"].best_estimator_\n",
    "best_params_pipe = pipe.named_steps[\"model\"].best_params_\n",
    "best_score_pipe = -pipe.named_steps[\"model\"].best_score_\n",
    "\n",
    "best_model_whpo = pipe_whpo.named_steps['model'].best_estimator_\n",
    "best_params_whpo = pipe_whpo.named_steps['model'].best_params_\n",
    "score_whpo = pipe_whpo.named_steps['model'].best_score_\n",
    "\n",
    "print(\"With scaler <-----> Without scaler <-----> Without HPO\")\n",
    "print(f\"Best model: {best_model_pipe} <------> {best_model} <------> {best_model_whpo}\")\n",
    "print(f\"Best params: {best_params_pipe} <------> {best_params} <------> {best_params_whpo}\")\n",
    "print(f\"Best score: {best_score_pipe} <------> {best_score} <------> {score_whpo}\")\n",
    "print(f\"Time taken to train the DecisionTreeRegressor Model: {b-a} seconds <-----> {c-b} seconds <------> {d-c} seconds\")\n",
    "\n",
    "graph_data[\"KNN\"] = [best_score_pipe, b-a]\n",
    "\n",
    "\n",
    "# Resultado con data sin filtrar\n",
    "# Mejor modelo: KNeighborsRegressor(leaf_size=1, n_neighbors=35, p=1) <-----------> KNeighborsRegressor(leaf_size=1, n_neighbors=15, p=1, weights='distance')\n",
    "# Mejorres parámetros: {'algorithm': 'auto', 'leaf_size': 1, 'n_neighbors': 35, 'p': 1, 'weights': 'uniform'} <-----------> {'algorithm': 'auto', 'leaf_size': 1, 'n_neighbors': 15, 'p': 1, 'weights': 'distance'}\n",
    "# Score: 593.643591243938 <-----------> 412.17437045226825\n",
    "# Time taken to train the KNeighborsRegressor Model with scaler: 30.788731813430786 seconds\n",
    "# Time taken to train the KNeighborsRegressor Model without scaler: 12.302868366241455 seconds\n",
    "\n",
    "# Resultado con data filtrada\n",
    "# Mejor modelo: KNeighborsRegressor(leaf_size=1, n_neighbors=35, p=1) <-----------> KNeighborsRegressor(leaf_size=1, n_neighbors=15, p=1, weights='distance')\n",
    "# Mejorres parámetros: {'algorithm': 'auto', 'leaf_size': 1, 'n_neighbors': 35, 'p': 1, 'weights': 'uniform'} <-----------> {'algorithm': 'auto', 'leaf_size': 1, 'n_neighbors': 15, 'p': 1, 'weights': 'distance'}\n",
    "# Score: 593.643591243938 <-----------> 412.17437045226825\n",
    "# Time taken to train the KNeighborsRegressor Model with scaler: 12.555383205413818 seconds\n",
    "# Time taken to train the KNeighborsRegressor Model without scaler: 5.734422922134399 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bcdeee",
   "metadata": {},
   "source": [
    "Como hemos podido observar, el uso del scaler cambia totalmente el resultado obtenido, convirtiendose así en el mejor modelo hasta la fecha y el más probable para el entrenamiendo final, pero, antes de apresurarnos, probaremos con **SVR**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4ea393",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = TimeSeriesSplit()\n",
    "\n",
    "param_grid = {\n",
    "    'C': range(705, 720),  # Regularization parameter\n",
    "    'epsilon': [0.1, 0.2, 0.3], # Controls the width of the margin around the regression line within which no penalty is incurred.\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],  # Kernel type\n",
    "    'degree': [1, 2, 3],  # Degree of the polynomial kernel (only for 'poly' kernel)\n",
    "    'gamma': ['scale', 'auto']  # Kernel coefficient for 'rbf', 'poly', and 'sigmoid'\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(SVR(),\n",
    "                            param_grid,\n",
    "                            cv=cv, \n",
    "                            n_jobs=-1, \n",
    "                            scoring='neg_root_mean_squared_error')\n",
    "\n",
    "without_hpo = GridSearchCV(SVR(),\n",
    "                           {}, cv=cv, n_jobs=-1, scoring='neg_root_mean_squared_error')\n",
    "\n",
    "\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('scaler', PowerTransformer()),\n",
    "    ('model', grid_search)\n",
    "])\n",
    "\n",
    "pipe_whpo = Pipeline([\n",
    "    ('scaler', PowerTransformer()),\n",
    "    ('model', without_hpo)\n",
    "])\n",
    "\n",
    "\n",
    "a = time.time()\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe_whpo.fit(X_train, y_train)\n",
    "c = time.time()\n",
    "\n",
    "best_params = pipe.named_steps['model'].best_params_\n",
    "score = pipe.named_steps['model'].best_score_\n",
    "\n",
    "best_params_whpo = pipe_whpo.named_steps['model'].best_params_\n",
    "score_whpo = pipe_whpo.named_steps['model'].best_score_\n",
    "\n",
    "print(\"With scaler <-----> Without HPO\")\n",
    "print(f\"Best params: {best_params} <------> {best_params_whpo}\")\n",
    "print(f\"Best score: {-score} <------> {-score_whpo}\")\n",
    "print(f\"Time taken to train the DecisionTreeRegressor Model: {b-a} seconds <-----> {c-b} seconds\")\n",
    "\n",
    "\n",
    "graph_data[\"KNN\"] = [-pipe.named_steps['model'].best_score_, b-a]\n",
    "\n",
    "\n",
    "# time taken to run the code:\n",
    "# time = \"13m 46s\"\n",
    "# 384.2528063686719\n",
    "# best model: SVR(C=719, degree=1, gamma='auto')\n",
    "# best params: {'C': 719, 'degree': 1, 'epsilon': 0.1, 'gamma': 'auto', 'kernel': 'rbf'}\n",
    "# RMSE:  380.9578481516544\n",
    "\n",
    "# Resultado con data sin filtrar\n",
    "# 447.31691802522346\n",
    "# best model: SVR(C=719, degree=1, gamma='auto')\n",
    "# best params: {'C': 719, 'degree': 1, 'epsilon': 0.1, 'gamma': 'auto', 'kernel': 'rbf'}\n",
    "# RMSE:  380.9578440914255\n",
    "# Time taken to train the SVR: 266.7573239803314 seconds ~ 4.4 minutes\n",
    "\n",
    "# Resultado con data filtrada\n",
    "# 434.6028671711526\n",
    "# best model: SVR(C=719, degree=1, gamma='auto')\n",
    "# best params: {'C': 719, 'degree': 1, 'epsilon': 0.1, 'gamma': 'auto', 'kernel': 'rbf'}\n",
    "# RMSE:  380.9578440914255\n",
    "# Time taken to train the SVR: 284.6029191017151 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d630832432fb07e5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Y como podemos observar, los mejores modelos son SVR, KNN con la pipe creada y el Decision Tree regresor.\n",
    "\n",
    "A pesar de que el KNN se entrena en menos tiempo y tiene las segundas mejores métricas, usaremos el SVR para la versión final ya que consideramos que la diferencia de tiempo no es tan relevante como el beneficio que se obtiene. Y siguiendo las reglas previamente mencionadas, el tiempo nos importa un 20% y la precisión un 80%, por lo que \n",
    "(266)*0.2 + (380)*0.8 = 357.2\n",
    "(30)*0.2 + (412)*0.8 = 335.6"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Antes de entrenar el modelo final, vamos a hacer una comparación de todos los modelos hasta la fecha, tenemos dos diccionarios, una que va a estar \"hardcodeada\" para evitar tener que ejecutar todos los modelos, y otra que va a ser la que se guarde después de la ejecución de todos los modelos, usaremos las listas para mostrar una grafica comparando los tiempos y la precisión.\n",
    "\n",
    "Cabe destacar que las medidas de tiempo no son muy precisas ya que el número de hiperparámetros que probamos en diferentes modelos varía, y por lo tanto es normal que tarden más."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a6d00be549d3b27"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Datos de resultados\n",
    "results = {\"Dummy\": [663, 0.087], \"Decision_Tree_R\": [425, 19.37], \"Linear_Regression\": [549, 0.38], \"Lasso\": [566, 27.60], \"KNN\": [412, 30.78], \"SVR\": [380, 266.75]}\n",
    "\n",
    "#### Graficas generadas por ChatGPT ####\n",
    "\n",
    "# Extraer nombres de modelos y puntajes\n",
    "modelos = list(results.keys())\n",
    "scores = [score[0] for score in results.values()]\n",
    "tiempos = [tiempo[1] for tiempo in results.values()]\n",
    "\n",
    "# Crear el gráfico de barras\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Barra para los puntajes (RMSE)\n",
    "plt.barh(modelos, scores, color='skyblue', label='RMSE')\n",
    "\n",
    "# Barra para los tiempos de entrenamiento\n",
    "plt.barh(modelos, tiempos, color='salmon', alpha=0.7, label='Tiempo de entrenamiento (seg)')\n",
    "\n",
    "# Etiquetas y título\n",
    "plt.xlabel('Score / Tiempo de entrenamiento')\n",
    "plt.ylabel('Modelo')\n",
    "plt.title('Comparación de Puntajes y Tiempos de Entrenamiento')\n",
    "plt.legend()\n",
    "\n",
    "# Mostrar gráfico\n",
    "plt.gca().invert_yaxis()  # Invertir el eje y para mostrar el mejor modelo arriba\n",
    "plt.grid(axis='x')  # Añadir rejilla solo en el eje x para mayor claridad\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "56dbf77586bf501"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Y por último, vamos a preparar el modelo final para la competición:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "99e306ef70e427c6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8d445276626183",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Al final el modelo seleccionado es el SVR, la variante para problemas de regresión de SVM, este tipo de modelos se basan en maquinas de soporte vectorial cuyo objetivo es minimizar la suma de la diferencia entre las predicciones y los valores reales.\n",
    "\"\"\"\n",
    "pipe = Pipeline([\n",
    "    ('scaler', PowerTransformer()),\n",
    "    ('model', SVR(C=719, degree=1, epsilon=0.1, gamma='auto', kernel='rbf'))\n",
    "])\n",
    "\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "y_pred_pipe = pipe.predict(X_test)\n",
    "\n",
    "rmse_pipe = np.sqrt(mean_squared_error(y_test, y_pred_pipe))\n",
    "print(\"RMSE:\", rmse_pipe)\n",
    "\n",
    "final_pipe = Pipeline([\n",
    "    ('scaler', PowerTransformer()),\n",
    "    ('model', SVR(C=719, degree=1, epsilon=0.1, gamma='auto', kernel='rbf'))\n",
    "])\n",
    "\n",
    "final_pipe.fit(X, y)\n",
    "\n",
    "# Save the model to a file\n",
    "dump(final_pipe, 'best_model_svr.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Uso de ChatGPT en esta parte:\n",
    "\n",
    "\n",
    "Como bien está comentado en el código, ChatGPT se usó para generar la gráfica que muestra los tiempos de ejecución y las scorings de todos los modelos.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f0c8a8d9a5045ddb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
